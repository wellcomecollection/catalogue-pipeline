{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS profile for local development\n",
    "%env AWS_PROFILE=platform-developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac340c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oai_pmh_client.client import OAIClient\n",
    "import httpx\n",
    "import logging\n",
    "\n",
    "from utils.aws import get_ssm_parameter\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s - %(message)s\",\n",
    "    force=True,\n",
    ")\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARN)\n",
    "\n",
    "API_TOKEN_SSM_PARAMETER = \"/catalogue_pipeline/axiell_collections/oai_api_token\"\n",
    "API_URL_SSM_PARAMETER = \"/catalogue_pipeline/axiell_collections/oai_api_url\"\n",
    "\n",
    "API_TOKEN = get_ssm_parameter(API_TOKEN_SSM_PARAMETER)\n",
    "API_URL = get_ssm_parameter(API_URL_SSM_PARAMETER)\n",
    "\n",
    "HTTP_TIMEOUT = httpx.Timeout(10.0, read=60.0)  # connect/read timeout tuning\n",
    "OAI_REQUEST_RETRIES = 4\n",
    "OAI_REQUEST_BACKOFF = 0.75\n",
    "OAI_REQUEST_BACKOFF_MAX = 5.0\n",
    "\n",
    "class AuthenticatedHTTPXClient(httpx.Client):\n",
    "    def build_request(self, method, url, **kwargs):\n",
    "        # Add the API token as a query parameter to each request\n",
    "        params = kwargs.pop(\"params\", {})\n",
    "        params[\"token\"] = API_TOKEN\n",
    "        kwargs[\"params\"] = params\n",
    "        return super().build_request(method, url, **kwargs)\n",
    "    \n",
    "# Instantiate the authenticated HTTPX client\n",
    "httpx_client = AuthenticatedHTTPXClient(timeout=HTTP_TIMEOUT)\n",
    "\n",
    "# Create a client for the arXiv OAI-PMH endpoint.\n",
    "client = OAIClient(\n",
    "    API_URL,\n",
    "    client=httpx_client,\n",
    "    max_request_retries=OAI_REQUEST_RETRIES,\n",
    "    request_backoff_factor=OAI_REQUEST_BACKOFF,\n",
    "    request_max_backoff=OAI_REQUEST_BACKOFF_MAX,\n",
    "    redacted_query_params=[\"token\"],\n",
    ")\n",
    "\n",
    "# Get the repository's identity.\n",
    "identity = client.identify()\n",
    "print(identity)\n",
    "\n",
    "# List the available metadata formats.\n",
    "formats = client.list_metadata_formats()\n",
    "print(formats)\n",
    "\n",
    "# List the sets in the repository.\n",
    "sets = client.list_sets()\n",
    "print(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import importlib\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "from lxml import etree\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "from pyiceberg.exceptions import NamespaceAlreadyExistsError\n",
    "from pyiceberg.table import Table as IcebergTable\n",
    "\n",
    "import oai_pmh_client.client\n",
    "importlib.reload(oai_pmh_client.client)\n",
    "from oai_pmh_client.client import OAIClient\n",
    "from adapters.utils.window_store import (\n",
    "    IcebergWindowStore,\n",
    "    WINDOW_STATUS_PARTITION_SPEC,\n",
    "    WINDOW_STATUS_SCHEMA,\n",
    ")\n",
    "from adapters.utils.window_harvester import WindowHarvestManager\n",
    "\n",
    "# Reuse the authenticated HTTPX client from the previous cell\n",
    "client = OAIClient(\n",
    "    API_URL,\n",
    "    client=httpx_client,\n",
    "    max_request_retries=OAI_REQUEST_RETRIES,\n",
    "    request_backoff_factor=OAI_REQUEST_BACKOFF,\n",
    "    request_max_backoff=OAI_REQUEST_BACKOFF_MAX,\n",
    "    redacted_query_params=[\"token\"],\n",
    ")\n",
    "\n",
    "# ---------------------- configuration knobs ----------------------\n",
    "LOOKBACK_DAYS = 3  # default window range covers the past week\n",
    "WINDOW_MINUTES = 15  # length of each sub-window\n",
    "MAX_PARALLEL_REQUESTS = 3  # number of windows fetched in parallel\n",
    "MAX_WINDOWS = None  # set to an integer to limit processing during tests\n",
    "METADATA_PREFIX = \"oai_raw\"\n",
    "SET_SPEC = \"collect\"\n",
    "AUTO_RUN_PENDING_WINDOWS = True\n",
    "\n",
    "# ---------------------------- paths -------------------------------\n",
    "DATA_DIR = Path(\"data/records\")\n",
    "ICEBERG_ROOT = Path(\"data/iceberg\")\n",
    "ICEBERG_WAREHOUSE = ICEBERG_ROOT / \"warehouse\"\n",
    "ICEBERG_CATALOG = ICEBERG_ROOT / \"catalog.db\"\n",
    "ICEBERG_NAMESPACE = (\"harvest\",)\n",
    "ICEBERG_TABLE_NAME = \"window_status\"\n",
    "ICEBERG_CATALOG_NAME = \"window_status_catalog\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ICEBERG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "ICEBERG_WAREHOUSE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _schema_field_names(schema) -> list[str]:\n",
    "    return [field.name for field in schema.fields]\n",
    "\n",
    "\n",
    "EXPECTED_WINDOW_SCHEMA_FIELDS = _schema_field_names(WINDOW_STATUS_SCHEMA)\n",
    "\n",
    "\n",
    "def load_or_create_window_status_table() -> IcebergTable:\n",
    "    catalog = SqlCatalog(\n",
    "        name=ICEBERG_CATALOG_NAME,\n",
    "        uri=f\"sqlite:///{ICEBERG_CATALOG.resolve()}\",\n",
    "        warehouse=str(ICEBERG_WAREHOUSE.resolve()),\n",
    "    )\n",
    "    try:\n",
    "        catalog.create_namespace(ICEBERG_NAMESPACE)\n",
    "    except NamespaceAlreadyExistsError:\n",
    "        pass\n",
    "    identifier = (*ICEBERG_NAMESPACE, ICEBERG_TABLE_NAME)\n",
    "    if catalog.table_exists(identifier):\n",
    "        table = catalog.load_table(identifier)\n",
    "        existing_fields = _schema_field_names(table.schema())\n",
    "        if existing_fields != EXPECTED_WINDOW_SCHEMA_FIELDS:\n",
    "            print(\n",
    "                \"Existing window_status table schema is outdated. Dropping table to rebuild with latest schema...\"\n",
    "            )\n",
    "            catalog.drop_table(identifier)\n",
    "        else:\n",
    "            return table\n",
    "    return catalog.create_table(\n",
    "        identifier=identifier,\n",
    "        schema=WINDOW_STATUS_SCHEMA,\n",
    "        partition_spec=WINDOW_STATUS_PARTITION_SPEC,\n",
    "    )\n",
    "\n",
    "\n",
    "window_status_table = load_or_create_window_status_table()\n",
    "store = IcebergWindowStore(window_status_table)\n",
    "\n",
    "# ------------------------ helper funcs --------------------------\n",
    "def sanitize_identifier(identifier: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", identifier)\n",
    "\n",
    "\n",
    "def header_to_dict(header):\n",
    "    if header is None:\n",
    "        return None\n",
    "    return {\n",
    "        \"identifier\": header.identifier,\n",
    "        \"datestamp\": header.datestamp.isoformat() if header.datestamp else None,\n",
    "        \"set_specs\": header.set_specs,\n",
    "        \"is_deleted\": header.is_deleted,\n",
    "    }\n",
    "\n",
    "\n",
    "def record_to_payload(\n",
    "    record,\n",
    "    window_start: datetime,\n",
    "    window_end: datetime,\n",
    "    fallback_suffix: int,\n",
    "    identifier_override: str | None = None,\n",
    "):\n",
    "    header = record.header\n",
    "    identifier = (\n",
    "        identifier_override\n",
    "        if identifier_override is not None\n",
    "        else header.identifier if header else f\"no-header-{window_start.isoformat()}-{fallback_suffix}\"\n",
    "    )\n",
    "    metadata_xml = None\n",
    "    if record.metadata is not None:\n",
    "        metadata_xml = etree.tostring(record.metadata, encoding=\"unicode\", pretty_print=True)\n",
    "    return identifier, {\n",
    "        \"window\": {\n",
    "            \"from\": window_start.isoformat(),\n",
    "            \"until\": window_end.isoformat(),\n",
    "        },\n",
    "        \"header\": header_to_dict(header),\n",
    "        \"metadata_xml\": metadata_xml,\n",
    "    }\n",
    "\n",
    "\n",
    "def write_record_json(identifier: str, payload: dict):\n",
    "    target = DATA_DIR / f\"{sanitize_identifier(identifier)}.json\"\n",
    "    target.write_text(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "    return target\n",
    "\n",
    "\n",
    "def write_record_callback(identifier, record, window_start, window_end, fallback_index):\n",
    "    _, payload = record_to_payload(\n",
    "        record,\n",
    "        window_start,\n",
    "        window_end,\n",
    "        fallback_index,\n",
    "        identifier_override=identifier,\n",
    "    )\n",
    "    write_record_json(identifier, payload)\n",
    "\n",
    "\n",
    "harvester = WindowHarvestManager(\n",
    "    client=client,\n",
    "    store=store,\n",
    "    metadata_prefix=METADATA_PREFIX,\n",
    "    set_spec=SET_SPEC,\n",
    "    window_minutes=WINDOW_MINUTES,\n",
    "    max_parallel_requests=MAX_PARALLEL_REQUESTS,\n",
    "    record_callback=write_record_callback,\n",
    ")\n",
    "\n",
    "\n",
    "def harvest_pending_windows(\n",
    "    start_time: datetime | None = None,\n",
    "    end_time: datetime | None = None,\n",
    "    max_windows: int | None = MAX_WINDOWS,\n",
    "):\n",
    "    end_time = end_time or datetime.now(timezone.utc)\n",
    "    start_time = start_time or (end_time - timedelta(days=LOOKBACK_DAYS))\n",
    "    summaries = harvester.harvest_recent(\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        max_windows=max_windows,\n",
    "    )\n",
    "    total_windows = len(summaries)\n",
    "    if total_windows == 0:\n",
    "        print(\"No windows to process.\")\n",
    "        return []\n",
    "    for summary in summaries:\n",
    "        state = summary[\"state\"].upper()\n",
    "        record_total = len(summary.get(\"record_ids\", []))\n",
    "        print(\n",
    "            f\"[{state}] {summary['window_key']} -> {record_total} record(s) \"\n",
    "            f\"after {summary['attempts']} attempt(s)\"\n",
    "        )\n",
    "        if summary.get(\"last_error\") and state != \"SUCCESS\":\n",
    "            print(f\"    Last error: {summary['last_error']}\")\n",
    "    return summaries\n",
    "\n",
    "\n",
    "if AUTO_RUN_PENDING_WINDOWS:\n",
    "    harvest_pending_windows()\n",
    "else:\n",
    "    failed_count = len(harvester.failed_windows())\n",
    "    print(\"Harvest setup complete. Call harvest_pending_windows() when you're ready to process windows.\")\n",
    "    print(f\"Current failed windows recorded: {failed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRY_FAILED_WINDOWS = True  # toggle to True to perform retries after inspection\n",
    "MAX_FAILED_WINDOWS_TO_RETRY = None  # optionally cap the number retried in one pass\n",
    "\n",
    "failed_rows = sorted(harvester.failed_windows(), key=lambda row: row[\"window_start\"])\n",
    "if not failed_rows:\n",
    "    print(\"No failed windows recorded in Iceberg.\")\n",
    "else:\n",
    "    print(f\"Failed windows recorded: {len(failed_rows)}\")\n",
    "    preview = failed_rows[: min(5, len(failed_rows))]\n",
    "    for row in preview:\n",
    "        print(\n",
    "            f\"  {row['window_key']} | attempts={row['attempts']} | last_error={row.get('last_error')}\"\n",
    "        )\n",
    "\n",
    "    if RETRY_FAILED_WINDOWS:\n",
    "        to_retry = (\n",
    "            failed_rows\n",
    "            if MAX_FAILED_WINDOWS_TO_RETRY is None\n",
    "            else failed_rows[:MAX_FAILED_WINDOWS_TO_RETRY]\n",
    "        )\n",
    "        if not to_retry:\n",
    "            print(\"Retry toggle enabled but MAX_FAILED_WINDOWS_TO_RETRY limited the set to zero.\")\n",
    "        else:\n",
    "            print(f\"Retrying {len(to_retry)} failed windows...\")\n",
    "            summaries = harvester.retry_failed_windows(limit=len(to_retry))\n",
    "            for summary in summaries:\n",
    "                state = summary[\"state\"].upper()\n",
    "                record_total = len(summary.get(\"record_ids\", []))\n",
    "                print(\n",
    "                    f\"[{state}] {summary['window_key']} -> {record_total} record(s) after {summary['attempts']} attempt(s)\"\n",
    "                )\n",
    "                if summary.get(\"last_error\") and state != \"SUCCESS\":\n",
    "                    print(f\"    Last error: {summary['last_error']}\")\n",
    "\n",
    "        refreshed = harvester.failed_windows()\n",
    "        print(\n",
    "            f\"Post-retry failures remaining: {len(refreshed)}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Retries disabled; review the details above to decide on next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = harvester.coverage_report()\n",
    "if report.total_windows == 0:\n",
    "    print(\"No window activity has been recorded in Iceberg yet.\")\n",
    "else:\n",
    "    print(\"Window coverage overview:\")\n",
    "    print(f\"  Time span: {report.range_start.isoformat()} -> {report.range_end.isoformat()}\")\n",
    "    print(f\"  Total windows: {report.total_windows}\")\n",
    "    state_summary = \", \".join(f\"{state}={count}\" for state, count in report.state_counts.items())\n",
    "    print(f\"  State counts: {state_summary}\")\n",
    "    print(f\"  Total window coverage (hours): {report.coverage_hours:.2f}\")\n",
    "    if report.coverage_gaps:\n",
    "        print(\"  Coverage gaps detected:\")\n",
    "        for gap in report.coverage_gaps[:5]:\n",
    "            print(f\"    Gap between {gap.start.isoformat()} -> {gap.end.isoformat()}\")\n",
    "        if len(report.coverage_gaps) > 5:\n",
    "            print(f\"    ...and {len(report.coverage_gaps) - 5} more gaps\")\n",
    "    else:\n",
    "        print(\"  No coverage gaps detected between processed windows.\")\n",
    "\n",
    "    if report.failures:\n",
    "        print(f\"  Failures recorded in range: {len(report.failures)}\")\n",
    "        for failure in report.failures[:5]:\n",
    "            print(\n",
    "                f\"    {failure.window_key} | attempts={failure.attempts} | last_error={failure.last_error}\"\n",
    "            )\n",
    "        if len(report.failures) > 5:\n",
    "            print(f\"    ...and {len(report.failures) - 5} more failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221de6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catalogue_graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
