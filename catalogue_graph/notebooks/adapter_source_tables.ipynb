{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS profile for local development\n",
    "%env AWS_PROFILE=platform-developer\n",
    "\n",
    "from adapters.ebsco.helpers import build_adapter_table as build_adapter_table_ebsco\n",
    "from adapters.axiell.runtime import AXIELL_CONFIG\n",
    "from adapters.folio.runtime import FOLIO_CONFIG\n",
    "\n",
    "# If false will attempt to use a local Iceberg table instead of using the S3 Tables REST API\n",
    "USE_REST_API_TABLE = False\n",
    "\n",
    "# Load the Iceberg table via the S3 Tables Iceberg REST API\n",
    "ebsco_adapter_table = build_adapter_table_ebsco(use_rest_api_table=USE_REST_API_TABLE)\n",
    "axiell_adapter_table = AXIELL_CONFIG.build_adapter_table(use_rest_api_table=USE_REST_API_TABLE)\n",
    "folio_adapter_table = FOLIO_CONFIG.build_adapter_table(use_rest_api_table=USE_REST_API_TABLE)\n",
    "\n",
    "tables = {\n",
    "       \"ebsco_adapter_table\":  ebsco_adapter_table,\n",
    "       \"axiell_adapter_table\": axiell_adapter_table,\n",
    "       \"folio_adapter_table\":  folio_adapter_table,\n",
    "}\n",
    "\n",
    "print(f\"Adapter tables loaded\")\n",
    "\n",
    "for table in tables.values():\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise each adapter table: total records + earliest/latest last_modified\n",
    "# Note: computing earliest/latest requires scanning the `last_modified` column and may take a while on large tables.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "\n",
    "def _table_summary(name: str, t):\n",
    "    # Count is usually metadata-driven (fast-ish), but can still take time depending on table/files.\n",
    "    count = t.scan().count()\n",
    "    \n",
    "    earliest = None\n",
    "    latest = None\n",
    "    try:\n",
    "        lm = t.scan(selected_fields=(\"last_modified\",)).to_arrow()[\"last_modified\"]\n",
    "        # Drop nulls if present\n",
    "        earliest = pc.min(lm).as_py()\n",
    "        latest = pc.max(lm).as_py()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute earliest/latest last_modified for {name}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        \"table\": name,\n",
    "        \"records\": count,\n",
    "        \"earliest_last_modified\": earliest,\n",
    "        \"latest_last_modified\": latest,\n",
    "    }\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    [\n",
    "        _table_summary(name, table) for name, table in tables.items()\n",
    "    ]\n",
    ").sort_values(\"table\")\n",
    "\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aaba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters.utils.schemata import ARROW_FIELDS\n",
    "\n",
    "FIELD_NAMES = [field.name for field in ARROW_FIELDS]\n",
    "\n",
    "def get_sample(table, limit = 20):\n",
    "    # Retrieve the first N data rows (excluding any projection to keep all columns)\n",
    "    first_n = table.scan(\n",
    "        selected_fields=FIELD_NAMES,\n",
    "        limit=limit,\n",
    "    ).to_arrow()\n",
    "\n",
    "    return first_n.to_pandas()\n",
    "\n",
    "\n",
    "def get_record_by_id(table, record_id: str):\n",
    "    record = table.scan(\n",
    "        selected_fields=FIELD_NAMES,\n",
    "        row_filter=f\"id = '{record_id}'\",\n",
    "    ).to_arrow()\n",
    "    \n",
    "    return record\n",
    "\n",
    "\n",
    "table_samples = {\n",
    "    name: get_sample(table)\n",
    "    for name, table in tables.items()\n",
    "}\n",
    "\n",
    "for name, sample in table_samples.items():\n",
    "    print(name)\n",
    "    display(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e40e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Table to consult\n",
    "example_table_name = \"ebsco_adapter_table\"\n",
    "sample_table = tables.get(example_table_name)\n",
    "\n",
    "# Extract an ID from the sample dataset, modify this to extract your own\n",
    "sample_df = table_samples.get(example_table_name)\n",
    "example_id = sample_df[\"id\"].head(1).to_list()[0]\n",
    "# example_id = \"12345\"\n",
    "\n",
    "example_record = get_record_by_id(sample_table, example_id)\n",
    "\n",
    "display(example_record)\n",
    "\n",
    "# parse the XML content of the record and pretty print it\n",
    "xml_value = example_record[\"content\"].to_pylist()[0]\n",
    "\n",
    "root = ET.fromstring(xml_value)\n",
    "\n",
    "ET.indent(root)\n",
    "ET.dump(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the record and print the transformed output\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "from adapters.transformers.ebsco_transformer import EbscoTransformer\n",
    "from adapters.transformers.axiell_transformer import AxiellTransformer\n",
    "from adapters.transformers.marcxml_transformer import MarcXmlTransformer\n",
    "from adapters.utils.adapter_store import AdapterStore\n",
    "\n",
    "table_to_transform = {\n",
    "    \"ebsco_adapter_table\": EbscoTransformer,\n",
    "    \"axiell_adapter_table\": AxiellTransformer,\n",
    "    # \"folio_adapter_table\": FolioTransformer,  # TODO: Add when FOLIO transformer is implemented\n",
    "}\n",
    "\n",
    "# Strictly not needed, unless we are triggering with a changeset\n",
    "adapter_store = AdapterStore(table)\n",
    "changeset_ids = [\"dummy_changeset\"]\n",
    "\n",
    "table = tables.get(example_table_name)\n",
    "transformer: MarcXmlTransformer  = table_to_transform.get(example_table_name)(adapter_store, changeset_ids)\n",
    "\n",
    "transformed_record = list(transformer.transform(example_record.to_pylist()))\n",
    "print(json.dumps(transformed_record[0].model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51244b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Backup and delete utilities for pyiceberg tables\n",
    "\n",
    "BACKUP_DIR = Path(\"./data/backups\")\n",
    "\n",
    "\n",
    "def backup_table(table, table_name: str) -> Path:\n",
    "    \"\"\"Backup all rows from a table to a parquet file in ./data/backups.\"\"\"\n",
    "    BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_path = BACKUP_DIR / f\"{table_name}_{timestamp}.parquet\"\n",
    "\n",
    "    # Scan all data and write to parquet\n",
    "    arrow_table = table.scan().to_arrow()\n",
    "    row_count = arrow_table.num_rows\n",
    "    if row_count == 0:\n",
    "        print(f\"Table '{table_name}' is empty, nothing to backup.\")\n",
    "        return backup_path\n",
    "\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    pq.write_table(arrow_table, backup_path)\n",
    "    print(f\"Backed up {row_count} rows to: {backup_path}\")\n",
    "    return backup_path\n",
    "\n",
    "\n",
    "def restore_backup(table, backup_path: str | Path) -> int:\n",
    "    \"\"\"Restore rows from a parquet backup file to the table.\"\"\"\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    backup_path = Path(backup_path)\n",
    "    if not backup_path.exists():\n",
    "        raise FileNotFoundError(f\"Backup file not found: {backup_path}\")\n",
    "\n",
    "    arrow_table = pq.read_table(backup_path)\n",
    "    row_count = arrow_table.num_rows\n",
    "    if row_count == 0:\n",
    "        print(\"Backup file is empty, nothing to restore.\")\n",
    "        return 0\n",
    "\n",
    "    table.append(arrow_table)\n",
    "    print(f\"Restored {row_count} rows from: {backup_path}\")\n",
    "    return row_count\n",
    "\n",
    "\n",
    "def list_backups(table_name: str | None = None) -> list[Path]:\n",
    "    \"\"\"List available backup files, optionally filtered by table name.\"\"\"\n",
    "    if not BACKUP_DIR.exists():\n",
    "        return []\n",
    "    backups = sorted(BACKUP_DIR.glob(\"*.parquet\"), reverse=True)\n",
    "    if table_name:\n",
    "        backups = [b for b in backups if b.name.startswith(f\"{table_name}_\")]\n",
    "    for b in backups:\n",
    "        print(f\"  {b.name}\")\n",
    "    return backups\n",
    "\n",
    "\n",
    "def delete_all_rows(table, table_name: str, *, backup: bool = True, dry_run: bool = False):\n",
    "    \"\"\"Delete all rows from a table, with optional backup first.\n",
    "    \n",
    "    Args:\n",
    "        table: The pyiceberg table to delete from.\n",
    "        table_name: Name used for backup file prefix.\n",
    "        backup: If True, backup the table before deleting.\n",
    "        dry_run: If True, perform backup but skip the actual deletion.\n",
    "    \"\"\"\n",
    "    from pyiceberg.expressions import AlwaysTrue\n",
    "\n",
    "    before_count = table.scan().count()\n",
    "    print(f\"Rows before delete: {before_count}\")\n",
    "\n",
    "    if before_count == 0:\n",
    "        print(\"Table is already empty, nothing to delete.\")\n",
    "        return\n",
    "\n",
    "    if backup:\n",
    "        backup_table(table, table_name)\n",
    "\n",
    "    if dry_run:\n",
    "        print(\"[DRY RUN] Skipping deletion. Backup was created if enabled.\")\n",
    "        return\n",
    "\n",
    "    with table.transaction() as tx:\n",
    "        try:\n",
    "            tx.delete(delete_filter=AlwaysTrue())\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Row-level delete failed.\") from e\n",
    "\n",
    "    after_count = table.scan().count()\n",
    "    print(f\"Rows after delete:  {after_count}\")\n",
    "    assert after_count == 0, \"Delete operation failed: table not empty\"\n",
    "    print(\"All rows deleted successfully via row-level delete.\")\n",
    "\n",
    "# Example usage:\n",
    "table_to_target_name = \"axiell_adapter_table\"\n",
    "table_to_target = tables.get(table_to_target_name)\n",
    "\n",
    "# Set dry_run=False to actually delete\n",
    "# delete_all_rows(table_to_target, table_to_target_name, backup=True, dry_run=True) \n",
    "# list_backups(table_to_target_name)\n",
    "# restore_backup(table_to_target, f\"./data/backups/{table_to_target_name}_20260115_155259.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catalogue_graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
