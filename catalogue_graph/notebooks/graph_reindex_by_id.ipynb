{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649dae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS profile for local development\n",
    "%env AWS_PROFILE=platform-developer\n",
    "\n",
    "neptune_client = NeptuneClient(\"prod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ffc32",
   "metadata": {},
   "source": [
    "# Reindex by ID\n",
    "\n",
    "Extract, transform, and optionally index specific **works** or **concepts** by their canonical IDs.\n",
    "This bypasses the normal window-based pipeline and fetches items directly from the merged index / Neptune graph.\n",
    "\n",
    "## Prerequisites\n",
    "- Running in the `catalogue_graph` uv environment (`uv sync`)\n",
    "- `platform-developer` AWS profile configured\n",
    "- VPN access to Wellcome Collection network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228afef",
   "metadata": {},
   "source": [
    "## Works\n",
    "\n",
    "Extract specific works from the merged ES index, enrich with Neptune graph data (hierarchy, concepts), transform, and optionally index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bff5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "\n",
    "from ingestor.extractors.base_extractor import GraphBaseExtractor\n",
    "from ingestor.extractors.works_extractor import (\n",
    "    ExtractedWork,\n",
    "    GraphWorksExtractor,\n",
    "    VisibleExtractedWork,\n",
    "    get_related_works_query,\n",
    ")\n",
    "from ingestor.models.merged.work import MergedWork\n",
    "from ingestor.transformers.works_transformer import ElasticsearchWorksTransformer\n",
    "from models.events import BasePipelineEvent, PipelineIndexDates\n",
    "from sources.merged_works_source import MergedWorksSource\n",
    "\n",
    "from utils.elasticsearch import ElasticsearchMode, get_client\n",
    "from clients.neptune_client import NeptuneClient\n",
    "\n",
    "# === Configuration ===\n",
    "WORK_IDS = [\"tsayk6g3\"]  # Change these to the work IDs you want to process\n",
    "PIPELINE_DATE = \"2025-10-02\"\n",
    "MERGED_INDEX_DATE = \"2025-10-02\"  # Set if different from pipeline_date\n",
    "\n",
    "es_client = get_client(\"works_ingestor\", PIPELINE_DATE, \"public\")\n",
    "\n",
    "class NotebookWorksExtractor(GraphWorksExtractor):\n",
    "    \"\"\"Extract specific works by ID, bypassing the window-based ES stream.\"\"\"\n",
    "\n",
    "    def __init__(self, work_ids: list[str], pipeline_date: str, merged_index_date: str | None = None):\n",
    "        # Skip GraphWorksExtractor.__init__ (which creates a full MergedWorksSource)\n",
    "        # and call GraphBaseExtractor.__init__ directly\n",
    "        super(GraphWorksExtractor, self).__init__(neptune_client)\n",
    "        self._work_ids = work_ids\n",
    "        self._pipeline_date = pipeline_date\n",
    "        self._merged_index_date = merged_index_date\n",
    "        self.streamed_ids: set[str] = set()\n",
    "        self.related_ids: set[str] = set()\n",
    "\n",
    "    def _make_event(self) -> BasePipelineEvent:\n",
    "        return BasePipelineEvent(\n",
    "            pipeline_date=self._pipeline_date,\n",
    "            index_dates=PipelineIndexDates(merged=self._merged_index_date),\n",
    "        )\n",
    "\n",
    "    def extract_raw(self) -> Generator[ExtractedWork]:\n",
    "        event = self._make_event()\n",
    "        source = MergedWorksSource(\n",
    "            event=event,\n",
    "            query={\"ids\": {\"values\": self._work_ids}},\n",
    "            es_client=es_client,\n",
    "        )\n",
    "        works_stream = (MergedWork.from_raw_document(w) for w in source.stream_raw())\n",
    "        yield from self.process_es_works(works_stream)\n",
    "\n",
    "        # Process related works (ancestors/children) for hierarchy consistency\n",
    "        related_ids = list(self.related_ids.difference(self.streamed_ids))\n",
    "        if related_ids:\n",
    "            print(f\"Processing {len(related_ids)} related works (ancestors/children)\")\n",
    "            related_source = MergedWorksSource(\n",
    "                event=event,\n",
    "                query=get_related_works_query(related_ids),\n",
    "                es_client=es_client,\n",
    "            )\n",
    "            related_stream = (MergedWork.from_raw_document(w) for w in related_source.stream_raw())\n",
    "            yield from self.process_es_works(related_stream)\n",
    "\n",
    "\n",
    "class NotebookWorksTransformer(ElasticsearchWorksTransformer):\n",
    "    \"\"\"Transform specific works by ID without needing a full pipeline event.\"\"\"\n",
    "\n",
    "    def __init__(self, work_ids: list[str], pipeline_date: str, merged_index_date: str | None = None):\n",
    "        # Skip ElasticsearchWorksTransformer.__init__ (which creates its own extractor)\n",
    "        self.source = NotebookWorksExtractor(work_ids, pipeline_date, merged_index_date)\n",
    "\n",
    "\n",
    "transformer = NotebookWorksTransformer(WORK_IDS, PIPELINE_DATE, MERGED_INDEX_DATE)\n",
    "indexed_works = list(transformer.stream_es_documents())\n",
    "print(f\"Transformed {len(indexed_works)} works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e414ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display transformed works\n",
    "for work in indexed_works:\n",
    "    work_id = work.get_id()\n",
    "    work_type = work.type\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Work: {work_id} ({work_type})\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    if hasattr(work, \"display\"):\n",
    "        d = work.display\n",
    "        print(f\"  Title: {d.title}\")\n",
    "        if hasattr(d, \"subjects\"):\n",
    "            print(f\"  Subjects: {[s.label for s in d.subjects[:5]]}\")\n",
    "        if hasattr(d, \"genres\"):\n",
    "            print(f\"  Genres: {[g.label for g in d.genres[:5]]}\")\n",
    "\n",
    "    if hasattr(work, \"query\") and hasattr(work.query, \"concept_ids\"):\n",
    "        print(f\"  Concept IDs: {work.query.concept_ids[:10]}\")\n",
    "\n",
    "    if hasattr(work, \"debug\"):\n",
    "        print(f\"  Debug source: {work.debug.source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838cedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Index the transformed works into Elasticsearch\n",
    "# Uncomment and run to actually write to the index\n",
    "\n",
    "# import json\n",
    "# import elasticsearch.helpers\n",
    "# from utils.elasticsearch import get_client, get_standard_index_name\n",
    "\n",
    "# INDEX_DATE = \"2025-10-02\"  # Change to target index date\n",
    "# es_client = get_client(\"works_ingestor\", PIPELINE_DATE, \"public\")\n",
    "# index_name = get_standard_index_name(\"works-indexed\", INDEX_DATE)\n",
    "\n",
    "# def generate_operations(works):\n",
    "#     for work in works:\n",
    "#         source = json.loads(work.model_dump_json(exclude_none=True))\n",
    "#         yield {\"_index\": index_name, \"_id\": work.get_id(), \"_source\": source}\n",
    "\n",
    "# success_count, errors = elasticsearch.helpers.bulk(es_client, generate_operations(indexed_works))\n",
    "# print(f\"Indexed {success_count} works to {index_name}\")\n",
    "# if errors:\n",
    "#     print(f\"Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d52c1",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "Extract specific concepts from the Neptune graph (with same-as expansion and related concepts), transform, and optionally index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15034dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "\n",
    "from ingestor.extractors.concepts_extractor import GraphConceptsExtractor, CONCEPT_QUERY_PARAMS\n",
    "from ingestor.transformers.concepts_transformer import ElasticsearchConceptsTransformer\n",
    "\n",
    "\n",
    "class NotebookConceptsExtractor(GraphConceptsExtractor):\n",
    "    \"\"\"Bypasses the ES source and uses fixed concept IDs.\"\"\"\n",
    "\n",
    "    def __init__(self, concept_ids: list[str]):\n",
    "        super(GraphConceptsExtractor, self).__init__(neptune_client)\n",
    "        self.primary_map: dict[str, str] = {}\n",
    "        self.same_as_map: dict[str, list[str]] = {}\n",
    "        self.neptune_params = CONCEPT_QUERY_PARAMS\n",
    "        self._concept_ids = concept_ids\n",
    "\n",
    "    def get_concept_stream(self) -> Generator[set[str]]:\n",
    "        self._update_same_as_map(self._concept_ids)\n",
    "        full_batch = set()\n",
    "        for cid in self._concept_ids:\n",
    "            for same_as_id in self.get_same_as(cid):\n",
    "                full_batch.add(same_as_id)\n",
    "        yield full_batch\n",
    "\n",
    "\n",
    "class NotebookConceptsTransformer(ElasticsearchConceptsTransformer):\n",
    "    \"\"\"Transform specific concepts by ID without needing a full pipeline event.\"\"\"\n",
    "\n",
    "    def __init__(self, concept_ids: list[str], only_specified: bool = False):\n",
    "        self.source = NotebookConceptsExtractor(concept_ids)\n",
    "        self._only_specified = set(concept_ids) if only_specified else None\n",
    "\n",
    "    def stream_es_documents(self):\n",
    "        for doc in super().stream_es_documents():\n",
    "            if self._only_specified and doc.get_id() not in self._only_specified:\n",
    "                continue\n",
    "            yield doc\n",
    "\n",
    "\n",
    "# === Configuration ===\n",
    "# CONCEPT_IDS = [\n",
    "#     \"zbus63qt\", \"dujvfptt\", \"u33bzxsb\", \"euehm7ng\", \"ec77rqzq\", \"g6f9sn7t\",\n",
    "#     \"bun3pg62\", \"cqm7r9pj\", \"d4r983x6\", \"ce7rratv\", \"a4wyrvq2\", \"ky24m9en\",\n",
    "#     \"hv3ueb5k\", \"qj5kj8rz\", \"rasp7aye\", \"byqnbpfc\", \"pestkwqm\", \"nns7bsba\",\n",
    "#     \"e8kur96g\", \"gk2eca5r\", \"up98mqb8\", \"usgkq8dj\", \"c24wmx3e\", \"umqzyxwk\",\n",
    "#     \"q7c2xvdk\", \"w7yp9m3v\", \"rtwg3paj\", \"d5ghwutb\", \"a7gmt7ff\", \"caew98cx\",\n",
    "#     \"h8fuyw3g\", \"vcqcqced\", \"u4y59z2p\", \"zwu7frtk\", \"nb2nvbwj\", \"khvwwfrk\",\n",
    "# ]\n",
    "CONCEPT_IDS=[\"w7yp9m3v\"]\n",
    "ONLY_SPECIFIED = True  # Set to False to also index the same-as concepts\n",
    "\n",
    "concepts_transformer = NotebookConceptsTransformer(CONCEPT_IDS, only_specified=ONLY_SPECIFIED)\n",
    "indexed_concepts = list(concepts_transformer.stream_es_documents())\n",
    "print(f\"Transformed {len(indexed_concepts)} concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3177da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display transformed concepts\n",
    "for concept in indexed_concepts:\n",
    "    concept_id = concept.get_id()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Concept: {concept_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    d = concept.display\n",
    "    print(f\"  Label: {d.label}\")\n",
    "    print(f\"  Display Label: {d.displayLabel}\")\n",
    "    print(f\"  Type: {d.type}\")\n",
    "    print(f\"  Alt Labels: {d.alternativeLabels[:5]}\")\n",
    "    print(f\"  Same As: {d.sameAs}\")\n",
    "    if d.description:\n",
    "        print(f\"  Description: {d.description.text[:200]}\")\n",
    "    if d.displayImages:\n",
    "        print(f\"  Images: {[img.url for img in d.displayImages[:3]]}\")\n",
    "\n",
    "    rc = d.relatedConcepts\n",
    "    for field_name in [\"relatedTo\", \"narrowerThan\", \"broaderThan\", \"people\", \"relatedTopics\"]:\n",
    "        items = getattr(rc, field_name, [])\n",
    "        if items:\n",
    "            print(f\"  {field_name}: {[f'{c.label} ({c.id})' for c in items[:5]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869144c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "import elasticsearch.helpers\n",
    "from utils.elasticsearch import get_client, get_standard_index_name\n",
    "\n",
    "# CONCEPTS_INDEX_DATE = \"2026-02-16\"  # Change to target index date\n",
    "# CONCEPTS_PIPELINE_DATE = \"2025-10-02\"\n",
    "# concepts_es_client = get_client(\"concepts_ingestor_rkenny_test\", CONCEPTS_PIPELINE_DATE, \"public\")\n",
    "# concepts_index_name = get_standard_index_name(\"concepts-indexed\", CONCEPTS_INDEX_DATE)\n",
    "\n",
    "# def generate_concept_operations(concepts):\n",
    "#     for c in concepts:\n",
    "#         source = json.loads(c.model_dump_json(exclude_none=True))\n",
    "#         yield {\"_index\": concepts_index_name, \"_id\": c.get_id(), \"_source\": source}\n",
    "\n",
    "# success_count, errors = elasticsearch.helpers.bulk(\n",
    "#     concepts_es_client,\n",
    "#     generate_concept_operations(indexed_concepts),\n",
    "#     raise_on_error=False,\n",
    "# )\n",
    "# print(f\"Indexed {success_count} concepts to {concepts_index_name}\")\n",
    "# if errors:\n",
    "#     print(f\"{len(errors)} error(s):\")\n",
    "#     for err in errors:\n",
    "#         pprint.pprint(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3ed76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catalogue_graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
