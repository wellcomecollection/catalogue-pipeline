{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8aa0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "ES_URL = \"http://localhost:9200\"\n",
    "INDEX_NAME = \"works-semantic-local\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95ce23f",
   "metadata": {},
   "source": [
    "## Load and Transform Works Data\n",
    "\n",
    "Load works from your JSON file and transform them to match the semantic search index schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b571c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "from pathlib import Path\n",
    "\n",
    "def transform_work(source_work: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Transform a source work document to match semantic search index schema.\"\"\"\n",
    "    \n",
    "    # Extract basic fields\n",
    "    transformed = {\n",
    "        \"id\": source_work.get(\"id\"),\n",
    "        \"title\": source_work.get(\"title\", \"\"),\n",
    "        \"description\": source_work.get(\"description\", \"\"),\n",
    "    }\n",
    "    \n",
    "    # Add semantic fields (copy from original)\n",
    "    transformed[\"titleSemantic\"] = transformed[\"title\"]\n",
    "    transformed[\"descriptionSemantic\"] = transformed[\"description\"]\n",
    "    \n",
    "    # Transform subjects if present\n",
    "    if \"subjects\" in source_work:\n",
    "        transformed[\"subjects\"] = [\n",
    "            {\n",
    "                \"label\": subject.get(\"label\", \"\"),\n",
    "                \"concepts\": [\n",
    "                    {\n",
    "                        \"id\": concept.get(\"id\"),\n",
    "                        \"label\": concept.get(\"label\", \"\")\n",
    "                    }\n",
    "                    for concept in subject.get(\"concepts\", [])\n",
    "                ]\n",
    "            }\n",
    "            for subject in source_work[\"subjects\"]\n",
    "        ]\n",
    "    \n",
    "    # Transform contributors if present\n",
    "    if \"contributors\" in source_work:\n",
    "        transformed[\"contributors\"] = [\n",
    "            {\n",
    "                \"agent\": {\n",
    "                    \"label\": contributor.get(\"agent\", {}).get(\"label\", \"\")\n",
    "                }\n",
    "            }\n",
    "            for contributor in source_work[\"contributors\"]\n",
    "        ]\n",
    "    \n",
    "    # Transform production dates if present\n",
    "    if \"production\" in source_work:\n",
    "        transformed[\"production\"] = [\n",
    "            {\n",
    "                \"dates\": [\n",
    "                    {\"label\": date.get(\"label\", \"\")}\n",
    "                    for date in prod.get(\"dates\", [])\n",
    "                ]\n",
    "            }\n",
    "            for prod in source_work[\"production\"]\n",
    "        ]\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "def load_works(file_path: str, max_docs: int = 1000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load and transform works from JSON file (handles both array and NDJSON).\"\"\"\n",
    "    works = []\n",
    "    \n",
    "    # Check file format\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_char = f.read(1)\n",
    "        is_json_array = first_char == '['\n",
    "    \n",
    "    if is_json_array:\n",
    "        # JSON array format - stream parse with ijson\n",
    "        print(f\"Loading from JSON array (max {max_docs} docs)...\")\n",
    "        with open(file_path, 'rb') as f:\n",
    "            parser = ijson.items(f, 'item')\n",
    "            for i, work in enumerate(parser):\n",
    "                if i >= max_docs:\n",
    "                    break\n",
    "                try:\n",
    "                    transformed = transform_work(work)\n",
    "                    works.append(transformed)\n",
    "                    if (i + 1) % 100 == 0:\n",
    "                        print(f\"  Loaded {i + 1} works...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Failed to transform work {i}: {e}\")\n",
    "    else:\n",
    "        # Newline-delimited JSON format\n",
    "        print(f\"Loading from NDJSON (max {max_docs} docs)...\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_docs:\n",
    "                    break\n",
    "                try:\n",
    "                    work = json.loads(line)\n",
    "                    transformed = transform_work(work)\n",
    "                    works.append(transformed)\n",
    "                    if (i + 1) % 100 == 0:\n",
    "                        print(f\"  Loaded {i + 1} works...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: Failed to parse line {i}: {e}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded and transformed {len(works)} works\")\n",
    "    return works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f70f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from NDJSON (max 100 docs)...\n",
      "  Loaded 100 works...\n",
      "\n",
      "Successfully loaded and transformed 100 works\n",
      "\n",
      "Sample transformed work:\n",
      "{\n",
      "  \"id\": \"wpe7g7wx\",\n",
      "  \"title\": \"Ethiopian magic scrolls / by Jacques Mercier ; [translated from the French by Richard Pevear].\",\n",
      "  \"description\": \"\",\n",
      "  \"titleSemantic\": \"Ethiopian magic scrolls / by Jacques Mercier ; [translated from the French by Richard Pevear].\",\n",
      "  \"descriptionSemantic\": \"\",\n",
      "  \"subjects\": [\n",
      "    {\n",
      "      \"label\": \"Ethiopian magic scrolls\",\n",
      "      \"concepts\": [\n",
      "        {\n",
      "          \"id\": \"nfh36ejb\",\n",
      "          \"label\": \"Ethiopian magic scrolls\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Illumination of books and manuscripts, Ethiopian\",\n",
      "      \"concepts\": [\n",
      "        {\n",
      "          \"id\": \"z9k738zt\",\n",
      "          \"label\": \"Illumination of books and manuscripts, Ethiopian\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"Illumination of books and manuscripts\",\n",
      "      \"concepts\": [\n",
      "        {\n",
      "          \"id\": \"suze5c8w\",\n",
      "          \"label\": \"Illumination of books and manuscripts\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"contributors\": [\n",
      "    {\n",
      "      \"agent\": {\n",
      "        \"label\": \"Mercier, Jacques, 1946-\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"agent\": {\n",
      "        \"label\": \"Richard Pevear\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"production\": [\n",
      "    {\n",
      "      \"dates\": [\n",
      "        {\n",
      "          \"label\": \"1979\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Specify your works JSON file path\n",
    "WORKS_FILE = \"./works.json\" \n",
    "MAX_WORKS = 100\n",
    "\n",
    "# Load and transform works\n",
    "works = load_works(WORKS_FILE, max_docs=MAX_WORKS)\n",
    "\n",
    "# Show a sample transformed work\n",
    "if works:\n",
    "    print(\"\\nSample transformed work:\")\n",
    "    print(json.dumps(works[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b819a4",
   "metadata": {},
   "source": [
    "## Bulk Index Works\n",
    "\n",
    "Index the transformed works into Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df0013bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 100 works using bulk API...\n",
      "  Processed 100/100 works...\n",
      "\n",
      "✓ Indexing complete:\n",
      "  Successful: 100\n",
      "  Failed: 0\n",
      "  Index refreshed - documents ready for search\n"
     ]
    }
   ],
   "source": [
    "# Bulk index the works using Elasticsearch bulk API\n",
    "print(f\"Indexing {len(works)} works using bulk API...\")\n",
    "\n",
    "def bulk_index_works(works: List[Dict[str, Any]], batch_size: int = 500):\n",
    "    \"\"\"Index works in batches using the bulk API.\"\"\"\n",
    "    total_indexed = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    for i in range(0, len(works), batch_size):\n",
    "        batch = works[i:i + batch_size]\n",
    "        \n",
    "        # Build bulk request body\n",
    "        bulk_body = []\n",
    "        for work in batch:\n",
    "            # Action line\n",
    "            bulk_body.append(json.dumps({\"index\": {\"_id\": work[\"id\"]}}))\n",
    "            # Document line\n",
    "            bulk_body.append(json.dumps(work))\n",
    "        \n",
    "        # Join with newlines and add trailing newline\n",
    "        bulk_data = \"\\n\".join(bulk_body) + \"\\n\"\n",
    "        \n",
    "        # Send bulk request\n",
    "        response = requests.post(\n",
    "            f\"{ES_URL}/{INDEX_NAME}/_bulk\",\n",
    "            headers={\"Content-Type\": \"application/x-ndjson\"},\n",
    "            data=bulk_data\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            if result.get(\"errors\"):\n",
    "                # Count failures\n",
    "                for item in result.get(\"items\", []):\n",
    "                    if item.get(\"index\", {}).get(\"status\") not in [200, 201]:\n",
    "                        total_failed += 1\n",
    "                    else:\n",
    "                        total_indexed += 1\n",
    "            else:\n",
    "                total_indexed += len(batch)\n",
    "            \n",
    "            print(f\"  Processed {min(i + batch_size, len(works))}/{len(works)} works...\")\n",
    "        else:\n",
    "            print(f\"✗ Bulk request failed: {response.status_code}\")\n",
    "            total_failed += len(batch)\n",
    "    \n",
    "    return total_indexed, total_failed\n",
    "\n",
    "# Perform bulk indexing\n",
    "indexed, failed = bulk_index_works(works, batch_size=500)\n",
    "\n",
    "# Refresh index to make documents searchable\n",
    "requests.post(f\"{ES_URL}/{INDEX_NAME}/_refresh\")\n",
    "\n",
    "print(f\"\\n✓ Indexing complete:\")\n",
    "print(f\"  Successful: {indexed}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(\"  Index refreshed - documents ready for search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af35f8",
   "metadata": {},
   "source": [
    "## Semantic Search Query\n",
    "\n",
    "Test semantic search with natural language queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query_text: str, field: str = \"titleSemantic\", size: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Perform semantic search using the semantic field.\"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"semantic\": {\n",
    "                \"field\": field,\n",
    "                \"query\": query_text\n",
    "            }\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{ES_URL}/{INDEX_NAME}/_search\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json=query\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "def print_results(results: Dict[str, Any], query: str):\n",
    "    \"\"\"Pretty print search results.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    hits = results.get('hits', {}).get('hits', [])\n",
    "    total = results.get('hits', {}).get('total', {}).get('value', 0)\n",
    "    \n",
    "    print(f\"Found {total} results\\n\")\n",
    "    \n",
    "    for i, hit in enumerate(hits, 1):\n",
    "        source = hit['_source']\n",
    "        score = hit['_score']\n",
    "        print(f\"{i}. [{source['id']}] {source['title']}\")\n",
    "        print(f\"   Score: {score:.4f}\")\n",
    "        print(f\"   {source['description'][:100]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319deea",
   "metadata": {},
   "source": [
    "## Experiment: Natural Language Queries\n",
    "\n",
    "Try different natural language queries to see how semantic search understands intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d92ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Looking for evolution/biology content\n",
    "results = semantic_search(\"books about how species evolve and adapt\", field=\"descriptionSemantic\")\n",
    "print_results(results, \"books about how species evolve and adapt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca447d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Looking for physics/cosmology content\n",
    "results = semantic_search(\"understanding the universe and physics\", field=\"descriptionSemantic\")\n",
    "print_results(results, \"understanding the universe and physics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Searching by concept rather than exact terms\n",
    "results = semantic_search(\"genetic information and heredity\", field=\"descriptionSemantic\")\n",
    "print_results(results, \"genetic information and heredity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7be1fd",
   "metadata": {},
   "source": [
    "## Compare: Semantic vs Keyword Search\n",
    "\n",
    "Compare semantic search with traditional keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3007f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(query_text: str, field: str = \"description\", size: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Perform traditional keyword search using match query.\"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                field: query_text\n",
    "            }\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{ES_URL}/{INDEX_NAME}/_search\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json=query\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Compare the two approaches\n",
    "test_query = \"genetic inheritance\"\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"SEMANTIC SEARCH\")\n",
    "print(\"#\" * 80)\n",
    "semantic_results = semantic_search(test_query, field=\"descriptionSemantic\")\n",
    "print_results(semantic_results, test_query)\n",
    "\n",
    "print(\"\\n\" + \"#\" * 80)\n",
    "print(\"KEYWORD SEARCH\")\n",
    "print(\"#\" * 80)\n",
    "keyword_results = keyword_search(test_query, field=\"description\")\n",
    "print_results(keyword_results, test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946e470",
   "metadata": {},
   "source": [
    "## Experiment: Hybrid Search\n",
    "\n",
    "Combine semantic and keyword search for best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220cc1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query_text: str, semantic_field: str = \"descriptionSemantic\", \n",
    "                  keyword_field: str = \"description\", size: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Combine semantic and keyword search.\"\"\"\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"semantic\": {\n",
    "                            \"field\": semantic_field,\n",
    "                            \"query\": query_text\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            keyword_field: {\n",
    "                                \"query\": query_text,\n",
    "                                \"boost\": 0.5  # Lower weight for keyword matching\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{ES_URL}/{INDEX_NAME}/_search\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json=query\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Test hybrid search\n",
    "results = hybrid_search(\"space and time\", semantic_field=\"descriptionSemantic\", keyword_field=\"description\")\n",
    "print_results(results, \"space and time (hybrid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408e55c",
   "metadata": {},
   "source": [
    "## Custom Query Experiments\n",
    "\n",
    "Try your own queries here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e146bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom query\n",
    "my_query = \"TODO: Enter your query here\"\n",
    "\n",
    "results = semantic_search(my_query, field=\"descriptionSemantic\")\n",
    "print_results(results, my_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf0e99",
   "metadata": {},
   "source": [
    "## Index Statistics\n",
    "\n",
    "Check the index status and document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc70c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index stats\n",
    "response = requests.get(f\"{ES_URL}/{INDEX_NAME}/_stats\")\n",
    "stats = response.json()\n",
    "\n",
    "index_stats = stats['indices'][INDEX_NAME]['total']\n",
    "print(\"Index Statistics:\")\n",
    "print(f\"  Documents: {index_stats['docs']['count']:,}\")\n",
    "print(f\"  Deleted: {index_stats['docs']['deleted']:,}\")\n",
    "print(f\"  Store size: {index_stats['store']['size_in_bytes'] / (1024**2):.2f} MB\")\n",
    "\n",
    "# Get a sample document\n",
    "response = requests.get(f\"{ES_URL}/{INDEX_NAME}/_search?size=1\")\n",
    "sample = response.json()['hits']['hits'][0]['_source']\n",
    "print(\"\\nSample document:\")\n",
    "print(json.dumps(sample, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-search-local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
