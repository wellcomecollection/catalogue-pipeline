{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344bf661",
   "metadata": {},
   "source": [
    "# Serverless Elasticsearch Semantic Search\n",
    "\n",
    "This notebook loads data into a serverless Elasticsearch project and experiments with semantic search using `semantic_text` fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb89765",
   "metadata": {},
   "source": [
    "## Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4bbe7937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key retrieved from AWS Secrets Manager\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "\n",
    "# Get API key from AWS Secrets Manager\n",
    "def get_elasticsearch_api_key():\n",
    "    \"\"\"Retrieve Elasticsearch API key from AWS Secrets Manager.\"\"\"\n",
    "    secret_name = \"agnes/elasticsearch/semantic-playground\"\n",
    "    region_name = \"eu-west-1\"\n",
    "    \n",
    "    session = boto3.session.Session(profile_name='platform-developer')\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "        secret = get_secret_value_response['SecretString']\n",
    "        return secret\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving secret: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "API_KEY = get_elasticsearch_api_key()\n",
    "print(\"API key retrieved from AWS Secrets Manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8883205",
   "metadata": {},
   "source": [
    "## Configure Elasticsearch Connection\n",
    "\n",
    "Configure your serverless Elasticsearch credentials. Update with your actual values from the Elasticsearch serverless console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b311f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch 8.11.0\n",
      "Cluster: b28f616a820849ab8b809ead5ba160f1\n"
     ]
    }
   ],
   "source": [
    "# Serverless Elasticsearch Configuration\n",
    "ES_ENDPOINT = \"https://semantic-playground-b28f61.es.eu-west-1.aws.elastic.cloud:443\"\n",
    "\n",
    "# Index configuration\n",
    "INDEX_NAME = \"works-semantic-v1\"\n",
    "\n",
    "# Setup headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"ApiKey {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def es_request(method: str, endpoint: str, data: dict = None) -> dict:\n",
    "    \"\"\"Make a request to Elasticsearch with proper authentication.\"\"\"\n",
    "    url = f\"{ES_ENDPOINT}/{endpoint}\"\n",
    "    if method == \"GET\":\n",
    "        response = requests.get(url, headers=headers)\n",
    "    elif method == \"POST\":\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "    elif method == \"PUT\":\n",
    "        response = requests.put(url, headers=headers, json=data)\n",
    "    elif method == \"DELETE\":\n",
    "        response = requests.delete(url, headers=headers)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    return response.json() if response.text else {}\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    info = es_request(\"GET\", \"\")\n",
    "    print(f\"Connected to Elasticsearch {info['version']['number']}\")\n",
    "    print(f\"Cluster: {info['cluster_name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2951c",
   "metadata": {},
   "source": [
    "## Create Inference Endpoint\n",
    "\n",
    "Set up the semantic text inference endpoint using Elasticsearch's ELSER model.\n",
    "This only needs to be run once for each embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589684ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference endpoint for semantic text using ELSER\n",
    "inference_config = {\n",
    "  \"service\": \"elser\",\n",
    "  \"service_settings\": {\n",
    "      \"num_allocations\": 1,\n",
    "      \"num_threads\": 1\n",
    "  }\n",
    "}\n",
    "\n",
    "try:\n",
    "    result = es_request(\"PUT\", \"_inference/sparse_embedding/elser-embeddings\", inference_config)\n",
    "    print(\"ELSER inference endpoint created successfully\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    if \"resource_already_exists_exception\" in str(e.response.text):\n",
    "        print(\"ELSER inference endpoint already exists\")\n",
    "    else:\n",
    "        print(f\"Error creating inference endpoint: {e}\")\n",
    "        print(e.response.text)\n",
    " \n",
    "\n",
    "# response\n",
    "# {\n",
    "#   \"inference_id\": \"elser-embeddings\",\n",
    "#   \"task_type\": \"sparse_embedding\",\n",
    "#   \"service\": \"elasticsearch\",\n",
    "#   \"service_settings\": {\n",
    "#     \"num_allocations\": 1,\n",
    "#     \"num_threads\": 1,\n",
    "#     \"model_id\": \".elser_model_2_linux-x86_64\",\n",
    "#     \"adaptive_allocations\": {\n",
    "#       \"enabled\": true,\n",
    "#       \"min_number_of_allocations\": 0,\n",
    "#       \"max_number_of_allocations\": 1\n",
    "#     }\n",
    "#   },\n",
    "#   \"chunking_settings\": {\n",
    "#     \"strategy\": \"sentence\",\n",
    "#     \"max_chunk_size\": 250,\n",
    "#     \"sentence_overlap\": 1\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d8fffe",
   "metadata": {},
   "source": [
    "## Load Sample Data\n",
    "\n",
    "Load the works.json file containing catalogue records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e57ea7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 works (limited to 10000)\n",
      "\n",
      "Sample work keys: ['production', 'physicalDescription', 'subjects', 'items', 'designation', 'workType', 'identifiers', 'formerFrequency', 'alternativeTitles', 'id', 'languages', 'partOf', 'genres', 'notes', 'holdings', 'title', 'type', 'contributors', 'images', 'availabilities', 'parts']\n",
      "\n",
      "Sample work: {'production': [{'label': 'New York : G. Braziller, 1979.', 'agents': [{'label': 'G. Braziller', 'type': 'Agent'}], 'dates': [{'label': '1979', 'type': 'Period'}], 'type': 'ProductionEvent', 'places': [{'label': 'New York', 'type': 'Place'}]}], 'physicalDescription': '118 pages : illustrations (some colour) ; 29 cm', 'subjects': [{'label': 'Ethiopian magic scrolls', 'concepts': [{'id': 'nfh36ejb', 'label': 'Ethiopian magic scrolls', 'identifiers': [{'value': 'sh85045154', 'type': 'Identifier', 'identifierType': {'id': 'lc-subjects', 'type': 'IdentifierType', 'label': 'Library of Congress Subject Headings (LCSH)'}}], 'type': 'Concept'}], 'identifiers': [{'value': 'sh85045154', 'type': 'Identifier', 'identifierType': {'id': 'lc-subjects', 'type': 'IdentifierType', 'label': 'Library of Congress Subject Headings (LCSH)'}}], 'id': 'nfh36ejb', 'type': 'Subject'}, {'label': 'Illumination of books and manuscripts, Ethiopian', 'concepts': [{'id': 'z9k738zt', 'label': 'Illumination of books and manuscripts, Ethiopian', 'identifiers': [{'value': 'sh88000554', 'type': 'Identifier', 'identifierType': {'id': 'lc-subjects', 'type': 'IdentifierType', 'label': 'Library of Congress Subject Headings (LCSH)'}}], 'type': 'Concept'}], 'identifiers': [{'value': 'sh88000554', 'type': 'Identifier', 'identifierType': {'id': 'lc-subjects', 'type': 'IdentifierType', 'label': 'Library of Congress Subject Headings (LCSH)'}}], 'id': 'z9k738zt', 'type': 'Subject'}, {'label': 'Illumination of books and manuscripts', 'concepts': [{'id': 'suze5c8w', 'label': 'Illumination of books and manuscripts', 'identifiers': [{'value': 'sh85064332', 'type': 'Identifier', 'identifierType': {'id': 'lc-subjects', 'type': 'IdentifierType', 'label': 'Library of Congress Subject Headings (LCSH)'}}], 'type': 'Concept'}], 'identifiers': [{'value': 'sh85064332', 'type': 'Identifier', 'identifierType': {'id': 'lc-subjects', 'type': 'IdentifierType', 'label': 'Library of Congress Subject Headings (LCSH)'}}], 'id': 'suze5c8w', 'type': 'Subject'}], 'items': [{'id': 'fastdrf7', 'identifiers': [{'value': 'i13059002', 'type': 'Identifier', 'identifierType': {'id': 'sierra-system-number', 'type': 'IdentifierType', 'label': 'Sierra system number'}}, {'value': '1305900', 'type': 'Identifier', 'identifierType': {'id': 'sierra-identifier', 'type': 'IdentifierType', 'label': 'Sierra identifier'}}], 'locations': [{'label': 'History of Medicine', 'accessConditions': [{'method': {'id': 'open-shelves', 'type': 'AccessMethod', 'label': 'Open shelves'}, 'status': {'id': 'temporarily-unavailable', 'type': 'AccessStatus', 'label': 'Temporarily unavailable'}, 'note': 'Item is in use by another reader. Please ask at Library Enquiry Desk.', 'type': 'AccessCondition'}], 'shelfmark': 'BVA.18', 'locationType': {'id': 'open-shelves', 'type': 'LocationType', 'label': 'Open shelves'}, 'type': 'PhysicalLocation'}], 'type': 'Item'}], 'designation': [], 'workType': {'id': 'a', 'type': 'Format', 'label': 'Books'}, 'identifiers': [{'value': 'b1291504x', 'type': 'Identifier', 'identifierType': {'id': 'sierra-system-number', 'type': 'IdentifierType', 'label': 'Sierra system number'}}, {'value': '1291504', 'type': 'Identifier', 'identifierType': {'id': 'sierra-identifier', 'type': 'IdentifierType', 'label': 'Sierra identifier'}}, {'value': '0807608963 :', 'type': 'Identifier', 'identifierType': {'id': 'isbn', 'type': 'IdentifierType', 'label': 'International Standard Book Number'}}, {'value': '0807608971', 'type': 'Identifier', 'identifierType': {'id': 'isbn', 'type': 'IdentifierType', 'label': 'International Standard Book Number'}}], 'formerFrequency': [], 'alternativeTitles': [], 'id': 'wpe7g7wx', 'languages': [{'id': 'eng', 'type': 'Language', 'label': 'English'}, {'id': 'fre', 'type': 'Language', 'label': 'French'}], 'partOf': [], 'genres': [], 'notes': [{'contents': ['Bibliography: p. 34-35.'], 'noteType': {'id': 'bibliographic-info', 'type': 'NoteType', 'label': 'Bibliographic information'}, 'type': 'Note'}], 'holdings': [], 'title': 'Ethiopian magic scrolls / by Jacques Mercier ; [translated from the French by Richard Pevear].', 'type': 'Work', 'contributors': [{'agent': {'id': 'baf5mr2c', 'label': 'Mercier, Jacques, 1946-', 'identifiers': [{'value': 'n78017769', 'type': 'Identifier', 'identifierType': {'id': 'lc-names', 'type': 'IdentifierType', 'label': 'Library of Congress Name authority records'}}], 'type': 'Person'}, 'roles': [], 'primary': True, 'type': 'Contributor'}, {'agent': {'id': 'n8vt35u5', 'label': 'Richard Pevear', 'identifiers': [{'value': 'n84149526', 'type': 'Identifier', 'identifierType': {'id': 'lc-names', 'type': 'IdentifierType', 'label': 'Library of Congress Name authority records'}}], 'type': 'Person'}, 'roles': [], 'primary': False, 'type': 'Contributor'}], 'images': [], 'availabilities': [{'id': 'open-shelves', 'type': 'Availability', 'label': 'Open shelves'}], 'parts': []}\n"
     ]
    }
   ],
   "source": [
    "# Configure sample size\n",
    "SAMPLE_SIZE = 10000  # Set to None to load all works\n",
    "\n",
    "def load_works(file_path: str, limit: int = None) -> List[Dict]:\n",
    "    \"\"\"Load works from JSON file (handles both array and NDJSON formats).\"\"\"\n",
    "    works = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_char = f.read(1)\n",
    "        f.seek(0)\n",
    "        \n",
    "        if first_char == '[':\n",
    "            # JSON array format\n",
    "            all_works = json.load(f)\n",
    "            works = all_works[:limit] if limit else all_works\n",
    "        else:\n",
    "            # NDJSON format\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    works.append(json.loads(line))\n",
    "                    if limit and len(works) >= limit:\n",
    "                        break\n",
    "    \n",
    "    return works\n",
    "\n",
    "# Load works\n",
    "works_file = Path(\"./works.json\")\n",
    "if works_file.exists():\n",
    "    works = load_works(str(works_file), SAMPLE_SIZE)\n",
    "    print(f\"Loaded {len(works)} works\" + (f\" (limited to {SAMPLE_SIZE})\" if SAMPLE_SIZE else \"\"))\n",
    "    print(f\"\\nSample work keys: {list(works[0].keys())}\")\n",
    "    print(f\"\\nSample work: {works[0]}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: {works_file} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93402c",
   "metadata": {},
   "source": [
    "## Create Index with Semantic Text Mapping\n",
    "\n",
    "Create an index with `semantic_text` fields for title and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d6f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mapping from file\n",
    "with open('mappings.works_semantic.json', 'r') as f:\n",
    "    mappings = json.load(f)\n",
    "\n",
    "index_config = {\n",
    "    \"mappings\": mappings\n",
    "}\n",
    "\n",
    "print(\"Loaded mapping with fields:\")\n",
    "for field_name in mappings['properties'].keys():\n",
    "    field_type = mappings['properties'][field_name].get('type', 'nested/object')\n",
    "    print(f\"  - {field_name}: {field_type}\")\n",
    "\n",
    "# Delete index if it exists\n",
    "# try:\n",
    "#     es_request(\"DELETE\", INDEX_NAME)\n",
    "#     print(f\"\\nDeleted existing index: {INDEX_NAME}\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Create the index\n",
    "try:\n",
    "    result = es_request(\"PUT\", INDEX_NAME, index_config)\n",
    "    print(f\"Created index: {INDEX_NAME}\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d1658",
   "metadata": {},
   "source": [
    "## Amend mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e96e71b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added titleDescriptionSemantic field to index\n",
      "{\n",
      "  \"acknowledged\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Add the new titleDescriptionSemantic field to existing index\n",
    "new_field_mapping = {\n",
    "    \"properties\": {\n",
    "        \"titleDescriptionSemantic\": {\n",
    "          \"type\": \"semantic_text\",\n",
    "          \"inference_id\": \"elser-embeddings\",\n",
    "          \"model_settings\": {\n",
    "            \"service\": \"elasticsearch\",\n",
    "            \"task_type\": \"sparse_embedding\"\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    result = es_request(\"PUT\", f\"{INDEX_NAME}/_mapping\", new_field_mapping)\n",
    "    print(\"Successfully added titleDescriptionSemantic field to index\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error updating mapping: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682b346",
   "metadata": {},
   "source": [
    "## Index Documents with Bulk API\n",
    "\n",
    "Transform and bulk index the works into Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "89f8fca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/400: 25 indexed, 0 failed\n",
      "Batch 2/400: 25 indexed, 0 failed\n",
      "Batch 3/400: 25 indexed, 0 failed\n",
      "Batch 4/400: 25 indexed, 0 failed\n",
      "Batch 5/400: 25 indexed, 0 failed\n",
      "Batch 6/400: 25 indexed, 0 failed\n",
      "Batch 7/400: 25 indexed, 0 failed\n",
      "Batch 8/400: 25 indexed, 0 failed\n",
      "Batch 9/400: 25 indexed, 0 failed\n",
      "Batch 10/400: 25 indexed, 0 failed\n",
      "Batch 11/400: 25 indexed, 0 failed\n",
      "Batch 12/400: 25 indexed, 0 failed\n",
      "Batch 13/400: 25 indexed, 0 failed\n",
      "Batch 14/400: 25 indexed, 0 failed\n",
      "Batch 15/400: 25 indexed, 0 failed\n",
      "Batch 16/400: 25 indexed, 0 failed\n",
      "Batch 17/400: 25 indexed, 0 failed\n",
      "Batch 18/400: 25 indexed, 0 failed\n",
      "Batch 19/400: 25 indexed, 0 failed\n",
      "Batch 20/400: 25 indexed, 0 failed\n",
      "Batch 21/400: 25 indexed, 0 failed\n",
      "Batch 22/400: 25 indexed, 0 failed\n",
      "Batch 23/400: 25 indexed, 0 failed\n",
      "Batch 24/400: 25 indexed, 0 failed\n",
      "Batch 25/400: 25 indexed, 0 failed\n",
      "Batch 26/400: 25 indexed, 0 failed\n",
      "Batch 27/400: 25 indexed, 0 failed\n",
      "Batch 28/400: 25 indexed, 0 failed\n",
      "Batch 29/400: 25 indexed, 0 failed\n",
      "Batch 30/400: 25 indexed, 0 failed\n",
      "Batch 31/400: 25 indexed, 0 failed\n",
      "Batch 32/400: 25 indexed, 0 failed\n",
      "Batch 33/400: 25 indexed, 0 failed\n",
      "Batch 34/400: 25 indexed, 0 failed\n",
      "Batch 35/400: 25 indexed, 0 failed\n",
      "Batch 36/400: 25 indexed, 0 failed\n",
      "Batch 37/400: 25 indexed, 0 failed\n",
      "Batch 38/400: 25 indexed, 0 failed\n",
      "Batch 39/400: 25 indexed, 0 failed\n",
      "Batch 40/400: 25 indexed, 0 failed\n",
      "Batch 41/400: 25 indexed, 0 failed\n",
      "Batch 42/400: 25 indexed, 0 failed\n",
      "Batch 43/400: 25 indexed, 0 failed\n",
      "Batch 44/400: 25 indexed, 0 failed\n",
      "Batch 45/400: 25 indexed, 0 failed\n",
      "Batch 46/400: 25 indexed, 0 failed\n",
      "Batch 47/400: 25 indexed, 0 failed\n",
      "Batch 48/400: 25 indexed, 0 failed\n",
      "Batch 49/400: 25 indexed, 0 failed\n",
      "Batch 50/400: 25 indexed, 0 failed\n",
      "Batch 51/400: 25 indexed, 0 failed\n",
      "Batch 52/400: 25 indexed, 0 failed\n",
      "Batch 53/400: 25 indexed, 0 failed\n",
      "Batch 54/400: 25 indexed, 0 failed\n",
      "Batch 55/400: 25 indexed, 0 failed\n",
      "Batch 56/400: 25 indexed, 0 failed\n",
      "Batch 57/400: 25 indexed, 0 failed\n",
      "Batch 58/400: 25 indexed, 0 failed\n",
      "Batch 59/400: 25 indexed, 0 failed\n",
      "Batch 60/400: 25 indexed, 0 failed\n",
      "Batch 61/400: 25 indexed, 0 failed\n",
      "Batch 62/400: 25 indexed, 0 failed\n",
      "Batch 63/400: 25 indexed, 0 failed\n",
      "Batch 64/400: 25 indexed, 0 failed\n",
      "Batch 65/400: 25 indexed, 0 failed\n",
      "Batch 66/400: 25 indexed, 0 failed\n",
      "Batch 67/400: 25 indexed, 0 failed\n",
      "Batch 68/400: 25 indexed, 0 failed\n",
      "Batch 69/400: 25 indexed, 0 failed\n",
      "Batch 70/400: 25 indexed, 0 failed\n",
      "Batch 71/400: 25 indexed, 0 failed\n",
      "Batch 72/400: 25 indexed, 0 failed\n",
      "Batch 73/400: 25 indexed, 0 failed\n",
      "Batch 74/400: 25 indexed, 0 failed\n",
      "Batch 75/400: 25 indexed, 0 failed\n",
      "Batch 76/400: 25 indexed, 0 failed\n",
      "Batch 77/400: 25 indexed, 0 failed\n",
      "Batch 78/400: 25 indexed, 0 failed\n",
      "Batch 79/400: 25 indexed, 0 failed\n",
      "Batch 80/400: 25 indexed, 0 failed\n",
      "Batch 81/400: 25 indexed, 0 failed\n",
      "Batch 82/400: 25 indexed, 0 failed\n",
      "Batch 83/400: 25 indexed, 0 failed\n",
      "Batch 84/400: 25 indexed, 0 failed\n",
      "Batch 85/400: 25 indexed, 0 failed\n",
      "Batch 86/400: 25 indexed, 0 failed\n",
      "Batch 87/400: 25 indexed, 0 failed\n",
      "Batch 88/400: 25 indexed, 0 failed\n",
      "Batch 89/400: 25 indexed, 0 failed\n",
      "Batch 90/400: 25 indexed, 0 failed\n",
      "Batch 91/400: 25 indexed, 0 failed\n",
      "Batch 92/400: 25 indexed, 0 failed\n",
      "Batch 93/400: 25 indexed, 0 failed\n",
      "Batch 94/400: 25 indexed, 0 failed\n",
      "Batch 95/400: 25 indexed, 0 failed\n",
      "Batch 96/400: 25 indexed, 0 failed\n",
      "Batch 97/400: 25 indexed, 0 failed\n",
      "Batch 98/400: 25 indexed, 0 failed\n",
      "Batch 99/400: 25 indexed, 0 failed\n",
      "Batch 100/400: 25 indexed, 0 failed\n",
      "Batch 101/400: 25 indexed, 0 failed\n",
      "Batch 102/400: 25 indexed, 0 failed\n",
      "Batch 103/400: 25 indexed, 0 failed\n",
      "Batch 104/400: 25 indexed, 0 failed\n",
      "Batch 105/400: 25 indexed, 0 failed\n",
      "Batch 106/400: 25 indexed, 0 failed\n",
      "Batch 107/400: 25 indexed, 0 failed\n",
      "Batch 108/400: 25 indexed, 0 failed\n",
      "Batch 109/400: 25 indexed, 0 failed\n",
      "Batch 110/400: 25 indexed, 0 failed\n",
      "Batch 111/400: 25 indexed, 0 failed\n",
      "Batch 112/400: 25 indexed, 0 failed\n",
      "Batch 113/400: 25 indexed, 0 failed\n",
      "Batch 114/400: 25 indexed, 0 failed\n",
      "Batch 115/400: 25 indexed, 0 failed\n",
      "Batch 116/400: 25 indexed, 0 failed\n",
      "Batch 117/400: 25 indexed, 0 failed\n",
      "Batch 118/400: 25 indexed, 0 failed\n",
      "Batch 119/400: 25 indexed, 0 failed\n",
      "Batch 120/400: 25 indexed, 0 failed\n",
      "Batch 121/400: 25 indexed, 0 failed\n",
      "Batch 122/400: 25 indexed, 0 failed\n",
      "Batch 123/400: 25 indexed, 0 failed\n",
      "Batch 124/400: 25 indexed, 0 failed\n",
      "Batch 125/400: 25 indexed, 0 failed\n",
      "Batch 126/400: 25 indexed, 0 failed\n",
      "Batch 127/400: 25 indexed, 0 failed\n",
      "Batch 128/400: 25 indexed, 0 failed\n",
      "Batch 129/400: 25 indexed, 0 failed\n",
      "Batch 130/400: 25 indexed, 0 failed\n",
      "Batch 131/400: 25 indexed, 0 failed\n",
      "Batch 132/400: 25 indexed, 0 failed\n",
      "Batch 133/400: 25 indexed, 0 failed\n",
      "Batch 134/400: 25 indexed, 0 failed\n",
      "Batch 135/400: 25 indexed, 0 failed\n",
      "Batch 136/400: 25 indexed, 0 failed\n",
      "Batch 137/400: 25 indexed, 0 failed\n",
      "Batch 138/400: 25 indexed, 0 failed\n",
      "Batch 139/400: 25 indexed, 0 failed\n",
      "Batch 140/400: 25 indexed, 0 failed\n",
      "Batch 141/400: 25 indexed, 0 failed\n",
      "Batch 142/400: 25 indexed, 0 failed\n",
      "Batch 143/400: 25 indexed, 0 failed\n",
      "Batch 144/400: 25 indexed, 0 failed\n",
      "Batch 145/400: 25 indexed, 0 failed\n",
      "Batch 146/400: 25 indexed, 0 failed\n",
      "Batch 147/400: 25 indexed, 0 failed\n",
      "Batch 148/400: 25 indexed, 0 failed\n",
      "Batch 149/400: 25 indexed, 0 failed\n",
      "Batch 150/400: 25 indexed, 0 failed\n",
      "Batch 151/400: 25 indexed, 0 failed\n",
      "Batch 152/400: 25 indexed, 0 failed\n",
      "Batch 153/400: 25 indexed, 0 failed\n",
      "Batch 154/400: 25 indexed, 0 failed\n",
      "Batch 155/400: 25 indexed, 0 failed\n",
      "Batch 156/400: 25 indexed, 0 failed\n",
      "Batch 157/400: 25 indexed, 0 failed\n",
      "Batch 158/400: 25 indexed, 0 failed\n",
      "Batch 159/400: 25 indexed, 0 failed\n",
      "Batch 160/400: 25 indexed, 0 failed\n",
      "Batch 161/400: 25 indexed, 0 failed\n",
      "Batch 162/400: 25 indexed, 0 failed\n",
      "Batch 163/400: 25 indexed, 0 failed\n",
      "Batch 164/400: 25 indexed, 0 failed\n",
      "Batch 165/400: 25 indexed, 0 failed\n",
      "Batch 166/400: 25 indexed, 0 failed\n",
      "Batch 167/400: 25 indexed, 0 failed\n",
      "Batch 168/400: 25 indexed, 0 failed\n",
      "Batch 169/400: 25 indexed, 0 failed\n",
      "Batch 170/400: 25 indexed, 0 failed\n",
      "Batch 171/400: 25 indexed, 0 failed\n",
      "Batch 172/400: 25 indexed, 0 failed\n",
      "Batch 173/400: 25 indexed, 0 failed\n",
      "Batch 174/400: 25 indexed, 0 failed\n",
      "Batch 175/400: 25 indexed, 0 failed\n",
      "Batch 176/400: 25 indexed, 0 failed\n",
      "Batch 177/400: 25 indexed, 0 failed\n",
      "Batch 178/400: 25 indexed, 0 failed\n",
      "Batch 179/400: 25 indexed, 0 failed\n",
      "Batch 180/400: 25 indexed, 0 failed\n",
      "Batch 181/400: 25 indexed, 0 failed\n",
      "Batch 182/400: 25 indexed, 0 failed\n",
      "Batch 183/400: 25 indexed, 0 failed\n",
      "Batch 184/400: 25 indexed, 0 failed\n",
      "Batch 185/400: 25 indexed, 0 failed\n",
      "Batch 186/400: 25 indexed, 0 failed\n",
      "Batch 187/400: 25 indexed, 0 failed\n",
      "Batch 188/400: 25 indexed, 0 failed\n",
      "Batch 189/400: 25 indexed, 0 failed\n",
      "Batch 190/400: 25 indexed, 0 failed\n",
      "Batch 191/400: 25 indexed, 0 failed\n",
      "Batch 192/400: 25 indexed, 0 failed\n",
      "Batch 193/400: 25 indexed, 0 failed\n",
      "Batch 194/400: 25 indexed, 0 failed\n",
      "Batch 195/400: 25 indexed, 0 failed\n",
      "Batch 196/400: 25 indexed, 0 failed\n",
      "Batch 197/400: 25 indexed, 0 failed\n",
      "Batch 198/400: 25 indexed, 0 failed\n",
      "Batch 199/400: 25 indexed, 0 failed\n",
      "Batch 200/400: 25 indexed, 0 failed\n",
      "Batch 201/400: 25 indexed, 0 failed\n",
      "Batch 202/400: 25 indexed, 0 failed\n",
      "Batch 203/400: 25 indexed, 0 failed\n",
      "Batch 204/400: 25 indexed, 0 failed\n",
      "Batch 205/400: 25 indexed, 0 failed\n",
      "Batch 206/400: 25 indexed, 0 failed\n",
      "Batch 207/400: 25 indexed, 0 failed\n",
      "Batch 208/400: 25 indexed, 0 failed\n",
      "Batch 209/400: 25 indexed, 0 failed\n",
      "Batch 210/400: 25 indexed, 0 failed\n",
      "Batch 211/400: 25 indexed, 0 failed\n",
      "Batch 212/400: 25 indexed, 0 failed\n",
      "Batch 213/400: 25 indexed, 0 failed\n",
      "Batch 214/400: 25 indexed, 0 failed\n",
      "Batch 215/400: 25 indexed, 0 failed\n",
      "Batch 216/400: 25 indexed, 0 failed\n",
      "Batch 217/400: 25 indexed, 0 failed\n",
      "Batch 218/400: 25 indexed, 0 failed\n",
      "Batch 219/400: 25 indexed, 0 failed\n",
      "Batch 220/400: 25 indexed, 0 failed\n",
      "Batch 221/400: 25 indexed, 0 failed\n",
      "Batch 222/400: 25 indexed, 0 failed\n",
      "Batch 223/400: 25 indexed, 0 failed\n",
      "Batch 224/400: 25 indexed, 0 failed\n",
      "Batch 225/400: 25 indexed, 0 failed\n",
      "Batch 226/400: 25 indexed, 0 failed\n",
      "Batch 227/400: 25 indexed, 0 failed\n",
      "Batch 228/400: 25 indexed, 0 failed\n",
      "Batch 229/400: 25 indexed, 0 failed\n",
      "Batch 230/400: 25 indexed, 0 failed\n",
      "Batch 231/400: 25 indexed, 0 failed\n",
      "Batch 232/400: 25 indexed, 0 failed\n",
      "Batch 233/400: 25 indexed, 0 failed\n",
      "Batch 234/400: 25 indexed, 0 failed\n",
      "Batch 235/400: 25 indexed, 0 failed\n",
      "Batch 236/400: 25 indexed, 0 failed\n",
      "Batch 237/400: 25 indexed, 0 failed\n",
      "Batch 238/400: 25 indexed, 0 failed\n",
      "Batch 239/400: 25 indexed, 0 failed\n",
      "Batch 240/400: 25 indexed, 0 failed\n",
      "Batch 241/400: 25 indexed, 0 failed\n",
      "Batch 242/400: 25 indexed, 0 failed\n",
      "Batch 243/400: 25 indexed, 0 failed\n",
      "Batch 244/400: 25 indexed, 0 failed\n",
      "Batch 245/400: 25 indexed, 0 failed\n",
      "Batch 246/400: 25 indexed, 0 failed\n",
      "Batch 247/400: 25 indexed, 0 failed\n",
      "Batch 248/400: 25 indexed, 0 failed\n",
      "Batch 249/400: 25 indexed, 0 failed\n",
      "Batch 250/400: 25 indexed, 0 failed\n",
      "Batch 251/400: 25 indexed, 0 failed\n",
      "Batch 252/400: 25 indexed, 0 failed\n",
      "Batch 253/400: 25 indexed, 0 failed\n",
      "Batch 254/400: 25 indexed, 0 failed\n",
      "Batch 255/400: 25 indexed, 0 failed\n",
      "Batch 256/400: 25 indexed, 0 failed\n",
      "Batch 257/400: 25 indexed, 0 failed\n",
      "Batch 258/400: 25 indexed, 0 failed\n",
      "Batch 259/400: 25 indexed, 0 failed\n",
      "Batch 260/400: 25 indexed, 0 failed\n",
      "Batch 261/400: 25 indexed, 0 failed\n",
      "Batch 262/400: 25 indexed, 0 failed\n",
      "Batch 263/400: 25 indexed, 0 failed\n",
      "Batch 264/400: 25 indexed, 0 failed\n",
      "Batch 265/400: 25 indexed, 0 failed\n",
      "Batch 266/400: 25 indexed, 0 failed\n",
      "Batch 267/400: 25 indexed, 0 failed\n",
      "Batch 268/400: 25 indexed, 0 failed\n",
      "Batch 269/400: 25 indexed, 0 failed\n",
      "Batch 270/400: 25 indexed, 0 failed\n",
      "Batch 271/400: 25 indexed, 0 failed\n",
      "Batch 272/400: 25 indexed, 0 failed\n",
      "Batch 273/400: 25 indexed, 0 failed\n",
      "Batch 274/400: 25 indexed, 0 failed\n",
      "Batch 275/400: 25 indexed, 0 failed\n",
      "Batch 276/400: 25 indexed, 0 failed\n",
      "Batch 277/400: 25 indexed, 0 failed\n",
      "Batch 278/400: 25 indexed, 0 failed\n",
      "Batch 279/400: 25 indexed, 0 failed\n",
      "Batch 280/400: 25 indexed, 0 failed\n",
      "Batch 281/400: 25 indexed, 0 failed\n",
      "Batch 282/400: 25 indexed, 0 failed\n",
      "Batch 283/400: 25 indexed, 0 failed\n",
      "Batch 284/400: 25 indexed, 0 failed\n",
      "Batch 285/400: 25 indexed, 0 failed\n",
      "Batch 286/400: 25 indexed, 0 failed\n",
      "Batch 287/400: 25 indexed, 0 failed\n",
      "Batch 288/400: 25 indexed, 0 failed\n",
      "Batch 289/400: 25 indexed, 0 failed\n",
      "Batch 290/400: 25 indexed, 0 failed\n",
      "Batch 291/400: 25 indexed, 0 failed\n",
      "Batch 292/400: 25 indexed, 0 failed\n",
      "Batch 293/400: 25 indexed, 0 failed\n",
      "Batch 294/400: 25 indexed, 0 failed\n",
      "Batch 295/400: 25 indexed, 0 failed\n",
      "Batch 296/400: 25 indexed, 0 failed\n",
      "Batch 297/400: 25 indexed, 0 failed\n",
      "Batch 298/400: 25 indexed, 0 failed\n",
      "Batch 299/400: 25 indexed, 0 failed\n",
      "Batch 300/400: 25 indexed, 0 failed\n",
      "Batch 301/400: 25 indexed, 0 failed\n",
      "Batch 302/400: 25 indexed, 0 failed\n",
      "Batch 303/400: 25 indexed, 0 failed\n",
      "Batch 304/400: 25 indexed, 0 failed\n",
      "Batch 305/400: 25 indexed, 0 failed\n",
      "Batch 306/400: 25 indexed, 0 failed\n",
      "Batch 307/400: 25 indexed, 0 failed\n",
      "Batch 308/400: 25 indexed, 0 failed\n",
      "Batch 309/400: 25 indexed, 0 failed\n",
      "Batch 310/400: 25 indexed, 0 failed\n",
      "Batch 311/400: 25 indexed, 0 failed\n",
      "Batch 312/400: 25 indexed, 0 failed\n",
      "Batch 313/400: 25 indexed, 0 failed\n",
      "Batch 314/400: 25 indexed, 0 failed\n",
      "Batch 315/400: 25 indexed, 0 failed\n",
      "Batch 316/400: 25 indexed, 0 failed\n",
      "Batch 317/400: 25 indexed, 0 failed\n",
      "Batch 318/400: 25 indexed, 0 failed\n",
      "Batch 319/400: 25 indexed, 0 failed\n",
      "Batch 320/400: 25 indexed, 0 failed\n",
      "Batch 321/400: 25 indexed, 0 failed\n",
      "Batch 322/400: 25 indexed, 0 failed\n",
      "Batch 323/400: 25 indexed, 0 failed\n",
      "Batch 324/400: 25 indexed, 0 failed\n",
      "Batch 325/400: 25 indexed, 0 failed\n",
      "Batch 326/400: 25 indexed, 0 failed\n",
      "Batch 327/400: 25 indexed, 0 failed\n",
      "Batch 328/400: 25 indexed, 0 failed\n",
      "Batch 329/400: 25 indexed, 0 failed\n",
      "Batch 330/400: 25 indexed, 0 failed\n",
      "Batch 331/400: 25 indexed, 0 failed\n",
      "Batch 332/400: 25 indexed, 0 failed\n",
      "Batch 333/400: 25 indexed, 0 failed\n",
      "Batch 334/400: 25 indexed, 0 failed\n",
      "Batch 335/400: 25 indexed, 0 failed\n",
      "Batch 336/400: 25 indexed, 0 failed\n",
      "Batch 337/400: 25 indexed, 0 failed\n",
      "Batch 338/400: 25 indexed, 0 failed\n",
      "Batch 339/400: 25 indexed, 0 failed\n",
      "Batch 340/400: 25 indexed, 0 failed\n",
      "Batch 341/400: 25 indexed, 0 failed\n",
      "Batch 342/400: 25 indexed, 0 failed\n",
      "Batch 343/400: 25 indexed, 0 failed\n",
      "Batch 344/400: 25 indexed, 0 failed\n",
      "Batch 345/400: 25 indexed, 0 failed\n",
      "Batch 346/400: 25 indexed, 0 failed\n",
      "Batch 347/400: 25 indexed, 0 failed\n",
      "Batch 348/400: 25 indexed, 0 failed\n",
      "Batch 349/400: 25 indexed, 0 failed\n",
      "Batch 350/400: 25 indexed, 0 failed\n",
      "Batch 351/400: 25 indexed, 0 failed\n",
      "Batch 352/400: 25 indexed, 0 failed\n",
      "Batch 353/400: 25 indexed, 0 failed\n",
      "Batch 354/400: 25 indexed, 0 failed\n",
      "Batch 355/400: 25 indexed, 0 failed\n",
      "Batch 356/400: 25 indexed, 0 failed\n",
      "Batch 357/400: 25 indexed, 0 failed\n",
      "Batch 358/400: 25 indexed, 0 failed\n",
      "Batch 359/400: 25 indexed, 0 failed\n",
      "Batch 360/400: 25 indexed, 0 failed\n",
      "Batch 361/400: 25 indexed, 0 failed\n",
      "Batch 362/400: 25 indexed, 0 failed\n",
      "Batch 363/400: 25 indexed, 0 failed\n",
      "Batch 364/400: 25 indexed, 0 failed\n",
      "Batch 365/400: 25 indexed, 0 failed\n",
      "Batch 366/400: 25 indexed, 0 failed\n",
      "Batch 367/400: 25 indexed, 0 failed\n",
      "Batch 368/400: 25 indexed, 0 failed\n",
      "Batch 369/400: 25 indexed, 0 failed\n",
      "Batch 370/400: 25 indexed, 0 failed\n",
      "Batch 371/400: 25 indexed, 0 failed\n",
      "Batch 372/400: 25 indexed, 0 failed\n",
      "Batch 373/400: 25 indexed, 0 failed\n",
      "Batch 374/400: 25 indexed, 0 failed\n",
      "Batch 375/400: 25 indexed, 0 failed\n",
      "Batch 376/400: 25 indexed, 0 failed\n",
      "Batch 377/400: 25 indexed, 0 failed\n",
      "Batch 378/400: 25 indexed, 0 failed\n",
      "Batch 379/400: 25 indexed, 0 failed\n",
      "Batch 380/400: 25 indexed, 0 failed\n",
      "Batch 381/400: 25 indexed, 0 failed\n",
      "Batch 382/400: 25 indexed, 0 failed\n",
      "Batch 383/400: 25 indexed, 0 failed\n",
      "Batch 384/400: 25 indexed, 0 failed\n",
      "Batch 385/400: 25 indexed, 0 failed\n",
      "Batch 386/400: 25 indexed, 0 failed\n",
      "Batch 387/400: 25 indexed, 0 failed\n",
      "Batch 388/400: 25 indexed, 0 failed\n",
      "Batch 389/400: 25 indexed, 0 failed\n",
      "Batch 390/400: 25 indexed, 0 failed\n",
      "Batch 391/400: 25 indexed, 0 failed\n",
      "Batch 392/400: 25 indexed, 0 failed\n",
      "Batch 393/400: 25 indexed, 0 failed\n",
      "Batch 394/400: 25 indexed, 0 failed\n",
      "Batch 395/400: 25 indexed, 0 failed\n",
      "Batch 396/400: 25 indexed, 0 failed\n",
      "Batch 397/400: 25 indexed, 0 failed\n",
      "Batch 398/400: 25 indexed, 0 failed\n",
      "Batch 399/400: 25 indexed, 0 failed\n",
      "Batch 400/400: 25 indexed, 0 failed\n",
      "\n",
      "============================================================\n",
      "Total: 10000 indexed, 0 failed in 1095.56s\n",
      "Rate: 9.1 docs/sec\n",
      "============================================================\n",
      "\n",
      "Bulk indexing completed\n"
     ]
    }
   ],
   "source": [
    "def transform_work(work: Dict) -> Dict:\n",
    "    \"\"\"Transform work to semantic search schema, matching mappings.works_semantic.json.\"\"\"\n",
    "    title = work.get('title', '')\n",
    "    description = work.get('description', '')\n",
    "    \n",
    "    # Concatenate title and description for semantic field\n",
    "    combined_text = f\"{title} {description}\".strip()\n",
    "    \n",
    "    # Transform subjects to only include mapped fields\n",
    "    subjects = []\n",
    "    for subject in work.get('subjects', []):\n",
    "        transformed_subject = {\n",
    "            'label': subject.get('label', '')\n",
    "        }\n",
    "        # Transform concepts to only include id and label\n",
    "        concepts = []\n",
    "        for concept in subject.get('concepts', []):\n",
    "            concepts.append({\n",
    "                'id': concept.get('id', ''),\n",
    "                'label': concept.get('label', '')\n",
    "            })\n",
    "        if concepts:\n",
    "            transformed_subject['concepts'] = concepts\n",
    "        subjects.append(transformed_subject)\n",
    "    \n",
    "    # Transform contributors to only include agent.label\n",
    "    contributors = []\n",
    "    for contributor in work.get('contributors', []):\n",
    "        agent = contributor.get('agent', {})\n",
    "        contributors.append({\n",
    "            'agent': {\n",
    "                'label': agent.get('label', '')\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Transform production to only include dates.label\n",
    "    production = []\n",
    "    for prod in work.get('production', []):\n",
    "        transformed_prod = {}\n",
    "        dates = []\n",
    "        for date in prod.get('dates', []):\n",
    "            dates.append({\n",
    "                'label': date.get('label', '')\n",
    "            })\n",
    "        if dates:\n",
    "            transformed_prod['dates'] = dates\n",
    "            production.append(transformed_prod)\n",
    "    \n",
    "    # Return only the fields defined in mappings.works_semantic.json\n",
    "    return {\n",
    "        'id': work.get('id', work.get('canonicalId', '')),\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'subjects': subjects,\n",
    "        'contributors': contributors,\n",
    "        'production': production,\n",
    "        'titleSemantic': title,\n",
    "        'descriptionSemantic': description,\n",
    "        'titleDescriptionSemantic': combined_text\n",
    "    }\n",
    "\n",
    "def bulk_index_works(works: List[Dict], batch_size: int = 25) -> Dict:\n",
    "    \"\"\"Bulk index works to Elasticsearch in batches.\"\"\"\n",
    "    import time\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    total_successes = 0\n",
    "    total_failures = 0\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(works), batch_size):\n",
    "        batch = works[i:i + batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        total_batches = (len(works) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Build bulk request for this batch\n",
    "        bulk_data = []\n",
    "        for work in batch:\n",
    "            doc = transform_work(work)\n",
    "            bulk_data.append(json.dumps({\"index\": {\"_index\": INDEX_NAME, \"_id\": doc['id']}}))\n",
    "            bulk_data.append(json.dumps(doc))\n",
    "        \n",
    "        bulk_body = \"\\n\".join(bulk_data) + \"\\n\"\n",
    "        \n",
    "        # Send bulk request with retry logic\n",
    "        url = f\"{ES_ENDPOINT}/_bulk\"\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    url,\n",
    "                    headers={**headers, \"Content-Type\": \"application/x-ndjson\"},\n",
    "                    data=bulk_body,\n",
    "                    timeout=120  # Increased timeout for ELSER processing\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                result = response.json()\n",
    "                \n",
    "                # Count successes and failures\n",
    "                items = result.get('items', [])\n",
    "                successes = sum(1 for item in items if item['index']['status'] in [200, 201])\n",
    "                failures = len(items) - successes\n",
    "                \n",
    "                total_successes += successes\n",
    "                total_failures += failures\n",
    "                \n",
    "                print(f\"Batch {batch_num}/{total_batches}: {successes} indexed, {failures} failed\")\n",
    "                \n",
    "                if failures > 0:\n",
    "                    # Show first error in this batch\n",
    "                    for item in items[:1]:\n",
    "                        if item['index']['status'] not in [200, 201]:\n",
    "                            print(f\"  Sample error: {item['index'].get('error', 'Unknown error')}\")\n",
    "                \n",
    "                break  # Success, exit retry loop\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Batch {batch_num} attempt {attempt + 1} failed: {e}. Retrying...\")\n",
    "                    time.sleep(3 * (attempt + 1))  # Progressive backoff: 3s, 6s, 9s\n",
    "                else:\n",
    "                    print(f\"Batch {batch_num} failed after {max_retries} attempts: {e}\")\n",
    "                    total_failures += len(batch)\n",
    "    \n",
    "    elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total: {total_successes} indexed, {total_failures} failed in {elapsed:.2f}s\")\n",
    "    print(f\"Rate: {total_successes / elapsed:.1f} docs/sec\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\"total_successes\": total_successes, \"total_failures\": total_failures}\n",
    "\n",
    "# Index the works\n",
    "if works:\n",
    "    result = bulk_index_works(works, batch_size=25)\n",
    "    print(f\"\\nBulk indexing completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce6454",
   "metadata": {},
   "source": [
    "## Verify Data Upload\n",
    "\n",
    "Check index stats and document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7dd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh index to make documents searchable\n",
    "es_request(\"POST\", f\"{INDEX_NAME}/_refresh\")\n",
    "\n",
    "# Get document count\n",
    "count_result = es_request(\"GET\", f\"{INDEX_NAME}/_count\")\n",
    "print(f\"Total documents in index: {count_result['count']}\")\n",
    "\n",
    "# Get index stats\n",
    "stats_result = es_request(\"GET\", f\"{INDEX_NAME}/_stats\")\n",
    "store_size = stats_result['indices'][INDEX_NAME]['total']['store']['size_in_bytes']\n",
    "print(f\"Index size: {store_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Check inference endpoint stats\n",
    "try:\n",
    "    inference_stats = es_request(\"GET\", \"_inference/_services/sparse_embedding/elser-embeddings/_stats\")\n",
    "    print(\"\\nInference stats:\")\n",
    "    print(json.dumps(inference_stats, indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Could not get inference stats: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8918dc",
   "metadata": {},
   "source": [
    "## Search Queries\n",
    "\n",
    "Template semantic, keywork and hybrid RFF queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d56a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_keyword(query: str, size: int = 10) -> Dict:\n",
    "    \"\"\"Search using traditional keyword search.\"\"\"\n",
    "    search_query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"title^3\", \"description\"]\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"id\", \"title\", \"description\"]\n",
    "    }\n",
    "    \n",
    "    return es_request(\"POST\", f\"{INDEX_NAME}/_search\", search_query)\n",
    "\n",
    "\n",
    "def search_semantic(query: str, fields: List[str] = None, size: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Search using semantic_text field(s) with the recommended match query.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        fields: List of semantic fields to search. Options: 'titleSemantic', 'descriptionSemantic', 'titleDescriptionSemantic'\n",
    "                If None, searches both titleSemantic and descriptionSemantic\n",
    "        size: Number of results to return\n",
    "    \"\"\"\n",
    "    if fields is None:\n",
    "        fields = [\"titleSemantic\", \"descriptionSemantic\"]\n",
    "    \n",
    "    search_query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\"match\": {field: {\"query\": query}}}\n",
    "                    for field in fields\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"id\", \"title\", \"description\"]\n",
    "    }\n",
    "    \n",
    "    return es_request(\"POST\", f\"{INDEX_NAME}/_search\", search_query)\n",
    "\n",
    "\n",
    "def rrf_retriever(query: str, size: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Use Elasticsearch retriever API with Reciprocal Rank Fusion (RRF) to combine keyword and semantic retrieval.\n",
    "    \"\"\"\n",
    "    rrf_query = {\n",
    "        \"retriever\": {\n",
    "            \"rrf\": {\n",
    "                \"retrievers\": [\n",
    "                    {\n",
    "                        \"standard\": {\n",
    "                            \"query\": {\n",
    "                                \"multi_match\": {\n",
    "                                    \"query\": query,\n",
    "                                    \"fields\": [\"title^3\", \"description\"]\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"standard\": {\n",
    "                            \"query\": {\n",
    "                                \"match\": {\n",
    "                                    \"titleDescriptionSemantic\": {\n",
    "                                        \"query\": query\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    return es_request(\"POST\", f\"{INDEX_NAME}/_search\", rrf_query)\n",
    "\n",
    "\n",
    "def filtered_semantic_rerank(query: str, filter: dict, size: int = 10, candidate_size: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Filter documents by nested fields, then rerank the filtered set using semantic relevance.\n",
    "    \n",
    "    Args:\n",
    "        query: The semantic search query string.\n",
    "        filter: A dict representing filters for nested fields. Supports:\n",
    "            - contributors.agent.label: term filter (string value)\n",
    "            - production.dates.label: range filter (dict value)\n",
    "        size: Number of final results to return.\n",
    "        candidate_size: Number of candidates to retrieve before reranking.\n",
    "    Returns:\n",
    "        Elasticsearch search results after semantic reranking.\n",
    "    \"\"\"\n",
    "    # Step 1: Build nested filter queries\n",
    "    filter_clauses = []\n",
    "    \n",
    "    for field_path, value in filter.items():\n",
    "        if field_path.startswith(\"contributors.\"):\n",
    "            # contributors is nested - always use term query\n",
    "            filter_clauses.append({\n",
    "                \"nested\": {\n",
    "                    \"path\": \"contributors\",\n",
    "                    \"query\": {\"term\": {field_path: value}}\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        elif field_path.startswith(\"production.\"):\n",
    "            # production.dates is doubly nested - always use range query\n",
    "            filter_clauses.append({\n",
    "                \"nested\": {\n",
    "                    \"path\": \"production.dates\",\n",
    "                    \"query\": {\"range\": {field_path: value}}\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Build filter query\n",
    "    filtered_query = {\n",
    "        \"size\": candidate_size,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": filter_clauses\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"id\", \"title\", \"description\"]\n",
    "    }\n",
    "    \n",
    "    filtered_results = es_request(\"POST\", f\"{INDEX_NAME}/_search\", filtered_query)\n",
    "    candidate_ids = [hit[\"_source\"][\"id\"] for hit in filtered_results[\"hits\"][\"hits\"]]\n",
    "    \n",
    "    if not candidate_ids:\n",
    "        return {\"hits\": {\"total\": {\"value\": 0}, \"hits\": []}}\n",
    "\n",
    "    # Step 2: Semantic rerank within candidates\n",
    "    rerank_query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\"terms\": {\"id\": candidate_ids}}\n",
    "                ],\n",
    "                \"should\": [\n",
    "                    {\"match\": {\"titleDescriptionSemantic\": {\"query\": query}}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"id\", \"title\", \"description\"]\n",
    "    }\n",
    "    return es_request(\"POST\", f\"{INDEX_NAME}/_search\", rerank_query)\n",
    "\n",
    "\n",
    "def print_results(results: Dict):\n",
    "    \"\"\"Pretty print search results.\"\"\"\n",
    "    hits = results['hits']['hits']\n",
    "    print(f\"\\nFound {results['hits']['total']['value']} results\\n\")\n",
    "    \n",
    "    for i, hit in enumerate(hits, 1):\n",
    "        source = hit['_source']\n",
    "        score = hit['_score']\n",
    "        doc_id = source.get('id', 'No ID')\n",
    "        work_url = f\"https://wellcomecollection.org/works/{doc_id}\"\n",
    "        print(f\"{i}. [Score: {score:.3f}] [{doc_id}] {source.get('title', 'No title')}\")\n",
    "        print(f\"   {work_url}\")\n",
    "        desc = source.get('description', '')\n",
    "        if desc:\n",
    "            print(f\"   {desc[:150]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d71e97a",
   "metadata": {},
   "source": [
    "## Compare Semantic Field Strategies\n",
    "\n",
    "Compare semantic search results across different field strategies:\n",
    "1. **KEYWORD BASELINE**: Uses keywork match, with a boost 3 on the title \n",
    "2. **SEPARATE TITLE AND DESCRIPTION FIELDS**: Use separate text vectors, title and description\n",
    "3. **CONCAT TITLE + DESCRIPTION**: Use a vector that concats title and description \n",
    "4. **NATIVE RRF**: Use both keyword and semantic search, with Reciprocal Rank Fusion\n",
    "5. **FILTER AND SEMANTIC RERANK**: Filters to return a list of relevant results then rescore based on semantic field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e86922c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "KEYWORD BASELINE\n",
      "================================================================================\n",
      "\n",
      "Found 12 results\n",
      "\n",
      "1. [Score: 30.536] [rr4gxybm] Environmental and experimental botany\n",
      "   https://wellcomecollection.org/works/rr4gxybm\n",
      "\n",
      "2. [Score: 27.989] [b2eyqe9f] Aquatic botany\n",
      "   https://wellcomecollection.org/works/b2eyqe9f\n",
      "\n",
      "3. [Score: 18.975] [p5vcv6uj] A synoptical compend of British botany ... arranged after the Linnean system / [John Kingston Galpine].\n",
      "   https://wellcomecollection.org/works/p5vcv6uj\n",
      "\n",
      "4. [Score: 18.752] [x33hq337] Cultivating women, cultivating science : Flora's daughters and botany in England, 1760-1860 / Ann B. Shteir.\n",
      "   https://wellcomecollection.org/works/x33hq337\n",
      "\n",
      "5. [Score: 17.778] [pcbzat7b] Percival, John (1865-1949), botanist, Emeritus Professor of Agriculture (Botany) at Reading University\n",
      "   https://wellcomecollection.org/works/pcbzat7b\n",
      "   1 autograph letter, signed, with envelope....\n",
      "\n",
      "6. [Score: 17.085] [duz67tp2] The development of biochemistry in England through botany and the brewing industry 1840-1890 / Neil Davies Morgan.\n",
      "   https://wellcomecollection.org/works/duz67tp2\n",
      "\n",
      "7. [Score: 16.939] [tucvky2z] Pristina medicamenta : ancient and medieval medical botany / Jerry Stannard ; edited by Katherine E. Stannard and Richard Kay.\n",
      "   https://wellcomecollection.org/works/tucvky2z\n",
      "\n",
      "8. [Score: 7.059] [ku725bk2] Letters to Hooker\n",
      "   https://wellcomecollection.org/works/ku725bk2\n",
      "   Two of these are invitations (nos. 39 and 39) and two are requests for information on Egyptian botany (no 40) and on the proper name for a shrub with ...\n",
      "\n",
      "9. [Score: 6.356] [qcrevkjd] Annals of the Missouri Botanical Garden\n",
      "   https://wellcomecollection.org/works/qcrevkjd\n",
      "   <p>Contains scientific contributions from the Missouri Botanical Garden and, from 1914-69, the Graduate Laboratory of the Henry Shaw School of Botany ...\n",
      "\n",
      "10. [Score: 5.078] [y77zw8q4] Miscellaneous notes\n",
      "   https://wellcomecollection.org/works/y77zw8q4\n",
      "   Notes from Lectures on Physiology by Dr. Gull. Notes from Lectures on Surgery by Mr. Poland. Notes on chemistry. Syllabus of botany. Taken at Guy's Ho...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SEPARATE TITLE AND DESCRIPTION FIELDS (titleSemantic + descriptionSemantic)\n",
      "================================================================================\n",
      "\n",
      "Found 2874 results\n",
      "\n",
      "1. [Score: 12.272] [hwsteeat] Plant science\n",
      "   https://wellcomecollection.org/works/hwsteeat\n",
      "   <p>\"An international journal of experimental plant biology.\"</p>...\n",
      "\n",
      "2. [Score: 9.850] [qcrevkjd] Annals of the Missouri Botanical Garden\n",
      "   https://wellcomecollection.org/works/qcrevkjd\n",
      "   <p>Contains scientific contributions from the Missouri Botanical Garden and, from 1914-69, the Graduate Laboratory of the Henry Shaw School of Botany ...\n",
      "\n",
      "3. [Score: 9.828] [mys4jcrq] Ward, Nathaniel Bagshaw, F.R.S. (1791-1868), botanist\n",
      "   https://wellcomecollection.org/works/mys4jcrq\n",
      "   5 autograph letters, signed; correspondents include the botanist William Gourlie (1815-1856) (no. 36)....\n",
      "\n",
      "4. [Score: 9.231] [cjq94rvm] Seemann, Berthold Carl (1825-1871), botanist and traveller\n",
      "   https://wellcomecollection.org/works/cjq94rvm\n",
      "   2 autograph letters in English, signed, including 1 to the botanist William Gourlie (1815-1856) (no. 4)....\n",
      "\n",
      "5. [Score: 8.789] [ugd9qq8y] Seeds of knowledge : early modern illustrated herbals / edited by Michael Jakob ; designed by Ewald Frick.\n",
      "   https://wellcomecollection.org/works/ugd9qq8y\n",
      "   <p>\"With sumptuous reproductions, Seeds of Knowledge highlights the extraordinary collection of 15th- to 17th-century European printed herbals of the ...\n",
      "\n",
      "6. [Score: 8.439] [jfhqn8ry] The histology of medicinal plants': review\n",
      "   https://wellcomecollection.org/works/jfhqn8ry\n",
      "   Holograph Review (incomplete) of 'The histology of medicinal plants' by W. Mansfield, New York...\n",
      "\n",
      "7. [Score: 8.295] [rr4gxybm] Environmental and experimental botany\n",
      "   https://wellcomecollection.org/works/rr4gxybm\n",
      "\n",
      "8. [Score: 7.282] [ku725bk2] Letters to Hooker\n",
      "   https://wellcomecollection.org/works/ku725bk2\n",
      "   Two of these are invitations (nos. 39 and 39) and two are requests for information on Egyptian botany (no 40) and on the proper name for a shrub with ...\n",
      "\n",
      "9. [Score: 7.216] [b2eyqe9f] Aquatic botany\n",
      "   https://wellcomecollection.org/works/b2eyqe9f\n",
      "\n",
      "10. [Score: 6.753] [ydv864we] Plants suitable for cultivation at Windsor\n",
      "   https://wellcomecollection.org/works/ydv864we\n",
      "   Plants suitable for cultivation at Windsor. Author's holograph MS....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONCAT TITLE + DESCRIPTION (titleDescriptionSemantic)\n",
      "================================================================================\n",
      "\n",
      "Found 2466 results\n",
      "\n",
      "1. [Score: 8.295] [rr4gxybm] Environmental and experimental botany\n",
      "   https://wellcomecollection.org/works/rr4gxybm\n",
      "\n",
      "2. [Score: 7.216] [b2eyqe9f] Aquatic botany\n",
      "   https://wellcomecollection.org/works/b2eyqe9f\n",
      "\n",
      "3. [Score: 6.626] [ugd9qq8y] Seeds of knowledge : early modern illustrated herbals / edited by Michael Jakob ; designed by Ewald Frick.\n",
      "   https://wellcomecollection.org/works/ugd9qq8y\n",
      "   <p>\"With sumptuous reproductions, Seeds of Knowledge highlights the extraordinary collection of 15th- to 17th-century European printed herbals of the ...\n",
      "\n",
      "4. [Score: 6.614] [pcbzat7b] Percival, John (1865-1949), botanist, Emeritus Professor of Agriculture (Botany) at Reading University\n",
      "   https://wellcomecollection.org/works/pcbzat7b\n",
      "   1 autograph letter, signed, with envelope....\n",
      "\n",
      "5. [Score: 5.982] [tucvky2z] Pristina medicamenta : ancient and medieval medical botany / Jerry Stannard ; edited by Katherine E. Stannard and Richard Kay.\n",
      "   https://wellcomecollection.org/works/tucvky2z\n",
      "\n",
      "6. [Score: 5.974] [p5vcv6uj] A synoptical compend of British botany ... arranged after the Linnean system / [John Kingston Galpine].\n",
      "   https://wellcomecollection.org/works/p5vcv6uj\n",
      "\n",
      "7. [Score: 5.954] [ku725bk2] Letters to Hooker\n",
      "   https://wellcomecollection.org/works/ku725bk2\n",
      "   Two of these are invitations (nos. 39 and 39) and two are requests for information on Egyptian botany (no 40) and on the proper name for a shrub with ...\n",
      "\n",
      "8. [Score: 5.893] [egs7e3gb] Trends in plant science\n",
      "   https://wellcomecollection.org/works/egs7e3gb\n",
      "\n",
      "9. [Score: 5.875] [mys4jcrq] Ward, Nathaniel Bagshaw, F.R.S. (1791-1868), botanist\n",
      "   https://wellcomecollection.org/works/mys4jcrq\n",
      "   5 autograph letters, signed; correspondents include the botanist William Gourlie (1815-1856) (no. 36)....\n",
      "\n",
      "10. [Score: 5.711] [yuaqwamm] Plant & cell physiology\n",
      "   https://wellcomecollection.org/works/yuaqwamm\n",
      "\n",
      "\n",
      "================================================================================\n",
      "NATIVE RRF (keyword + semantic)\n",
      "================================================================================\n",
      "\n",
      "Found 2466 results\n",
      "\n",
      "1. [Score: 0.033] [rr4gxybm] Environmental and experimental botany\n",
      "   https://wellcomecollection.org/works/rr4gxybm\n",
      "\n",
      "2. [Score: 0.032] [b2eyqe9f] Aquatic botany\n",
      "   https://wellcomecollection.org/works/b2eyqe9f\n",
      "\n",
      "3. [Score: 0.031] [p5vcv6uj] A synoptical compend of British botany ... arranged after the Linnean system / [John Kingston Galpine].\n",
      "   https://wellcomecollection.org/works/p5vcv6uj\n",
      "\n",
      "4. [Score: 0.031] [pcbzat7b] Percival, John (1865-1949), botanist, Emeritus Professor of Agriculture (Botany) at Reading University\n",
      "   https://wellcomecollection.org/works/pcbzat7b\n",
      "   1 autograph letter, signed, with envelope....\n",
      "\n",
      "5. [Score: 0.030] [tucvky2z] Pristina medicamenta : ancient and medieval medical botany / Jerry Stannard ; edited by Katherine E. Stannard and Richard Kay.\n",
      "   https://wellcomecollection.org/works/tucvky2z\n",
      "\n",
      "6. [Score: 0.030] [ku725bk2] Letters to Hooker\n",
      "   https://wellcomecollection.org/works/ku725bk2\n",
      "   Two of these are invitations (nos. 39 and 39) and two are requests for information on Egyptian botany (no 40) and on the proper name for a shrub with ...\n",
      "\n",
      "7. [Score: 0.016] [ugd9qq8y] Seeds of knowledge : early modern illustrated herbals / edited by Michael Jakob ; designed by Ewald Frick.\n",
      "   https://wellcomecollection.org/works/ugd9qq8y\n",
      "   <p>\"With sumptuous reproductions, Seeds of Knowledge highlights the extraordinary collection of 15th- to 17th-century European printed herbals of the ...\n",
      "\n",
      "8. [Score: 0.016] [x33hq337] Cultivating women, cultivating science : Flora's daughters and botany in England, 1760-1860 / Ann B. Shteir.\n",
      "   https://wellcomecollection.org/works/x33hq337\n",
      "\n",
      "9. [Score: 0.015] [duz67tp2] The development of biochemistry in England through botany and the brewing industry 1840-1890 / Neil Davies Morgan.\n",
      "   https://wellcomecollection.org/works/duz67tp2\n",
      "\n",
      "10. [Score: 0.015] [egs7e3gb] Trends in plant science\n",
      "   https://wellcomecollection.org/works/egs7e3gb\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FILTER AND SEMANTIC RERANK\n",
      "================================================================================\n",
      "\n",
      "Found 100 results\n",
      "\n",
      "1. [Score: 1.377] [mf472c5r] Mrs. Vesey Stockley (fl. 1902)\n",
      "   https://wellcomecollection.org/works/mf472c5r\n",
      "   2 autograph letters, signed, to Mrs. Stockley, 1 sent to her at the 16th Bengal Lancer lines in the Punjab containing recipes for marmalade (no.82), 1...\n",
      "\n",
      "2. [Score: 1.251] [qkf6ufxb] Journal of experimental marine biology and ecology\n",
      "   https://wellcomecollection.org/works/qkf6ufxb\n",
      "\n",
      "3. [Score: 1.230] [yw2w7gpu] Weiterer Beitrag zur Leishmania-Anmie / von G. Caronia.\n",
      "   https://wellcomecollection.org/works/yw2w7gpu\n",
      "\n",
      "4. [Score: 1.228] [yumbnbu9] The master book of herbalism / Paul Beyerl ; illustrations by Diana Greene.\n",
      "   https://wellcomecollection.org/works/yumbnbu9\n",
      "\n",
      "5. [Score: 1.148] [x9u3zd9g] Microbiology / Kenneth L. Burdon, with the collaboration of Robert P. Williams.\n",
      "   https://wellcomecollection.org/works/x9u3zd9g\n",
      "\n",
      "6. [Score: 0.995] [wzyyqgfk] Causes et traitements des rhumatismes chroniques / par R.-J. Weissenbach et F. Franon.\n",
      "   https://wellcomecollection.org/works/wzyyqgfk\n",
      "\n",
      "7. [Score: 0.685] [fccza9ww] Abstract of the Milroy lectures on kala-azar, its differentiation and its epidemiology. Lecture III, The life-history of the parasite, mode of infection, and prophylaxis / by Leonard Rogers.\n",
      "   https://wellcomecollection.org/works/fccza9ww\n",
      "\n",
      "8. [Score: 0.667] [w8jyz9c4] A practical treatise on the use of the microscope : including the different methods of preparing and examining animal, vegetable, and mineral structures / by John Quekett.\n",
      "   https://wellcomecollection.org/works/w8jyz9c4\n",
      "\n",
      "9. [Score: 0.576] [z8r3p4h4] International index of laboratory animals : giving sources of animals used in laboratories throughout the world / compiled and edited by Michael F.W. Festing.\n",
      "   https://wellcomecollection.org/works/z8r3p4h4\n",
      "\n",
      "10. [Score: 0.559] [uq6druur] Atomism, Poincar and Planck / H. Krips.\n",
      "   https://wellcomecollection.org/works/uq6druur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_query = \"botany\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEYWORD BASELINE\")\n",
    "print(\"=\" * 80)\n",
    "keyword_results = search_keyword(test_query, size=10)\n",
    "print_results(keyword_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SEPARATE TITLE AND DESCRIPTION FIELDS (titleSemantic + descriptionSemantic)\")\n",
    "print(\"=\" * 80)\n",
    "desc_results = search_semantic(test_query, fields=[\"titleSemantic\", \"descriptionSemantic\"], size=10)\n",
    "print_results(desc_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCAT TITLE + DESCRIPTION (titleDescriptionSemantic)\")\n",
    "print(\"=\" * 80)\n",
    "combined_results = search_semantic(test_query, fields=[\"titleDescriptionSemantic\"], size=10)\n",
    "print_results(combined_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NATIVE RRF (keyword + semantic)\")\n",
    "print(\"=\" * 80)\n",
    "rff_results = rff_retriever(test_query, size=10)\n",
    "print_results(rff_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FILTER AND SEMANTIC RERANK\")\n",
    "print(\"=\" * 80)\n",
    "reranked_results = filtered_semantic_rerank(\n",
    "    test_query, \n",
    "    filter={\"production.dates.label\": {\"gte\": \"1900\", \"lte\": \"1990\"}}, \n",
    "    # filter={\"contributors.agent.label\": \"Royal College of Surgeons of England\"}, \n",
    "    size=10\n",
    ")\n",
    "print_results(reranked_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55584af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-search-local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
